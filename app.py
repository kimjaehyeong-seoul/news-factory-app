# ğŸš€ ë‰´ìŠ¤íŒ©í† ë¦¬ - AI ê¸°ë°˜ ê¸°ì‚¬ ë¶„ì„ í”Œë«í¼ (ë„¤ì´ë²„ ë§í¬ í•˜ì´í¼ë§í¬ ì ìš©)
import streamlit as st
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import re
import os
import time
import hashlib
from pathlib import Path
import requests
import pickle
import json
from collections import Counter, defaultdict
import subprocess
import sys
import calendar

# ìë™ íŒ¨í‚¤ì§€ ì„¤ì¹˜ í•¨ìˆ˜
def install_package(package):
    """íŒ¨í‚¤ì§€ ìë™ ì„¤ì¹˜"""
    try:
        subprocess.check_call([sys.executable, "-m", "pip", "install", package])
        return True
    except subprocess.CalledProcessError:
        return False

# soynlp ê¸°ë°˜ í•œêµ­ì–´ ì²˜ë¦¬
def safe_import():
    """ì•ˆì „í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ import - soynlp í™œìš©"""
    global TfidfVectorizer, cosine_similarity, word_extractor, noun_extractor
    # í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬
    try:
        from sklearn.feature_extraction.text import TfidfVectorizer
        from sklearn.metrics.pairwise import cosine_similarity
    except ImportError:
        st.warning("âš ï¸ scikit-learnì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ìœ ì‚¬ë„ ë¶„ì„ ê¸°ëŠ¥ì´ ì œí•œë©ë‹ˆë‹¤.")
        TfidfVectorizer = None
        cosine_similarity = None
    
    # soynlp í•œêµ­ì–´ ì²˜ë¦¬ (1ìˆœìœ„ ì¶”ì²œ)
    try:
        from soynlp.word import WordExtractor
        from soynlp.noun import LRNounExtractor_v2
        word_extractor = WordExtractor()
        noun_extractor = LRNounExtractor_v2()
        st.success("âœ… soynlp í•œêµ­ì–´ ì²˜ë¦¬ ì—”ì§„ ë¡œë“œ ì„±ê³µ")
        return
    except ImportError:
        st.warning("âš ï¸ soynlp ì„¤ì¹˜ í•„ìš”: pip install soynlp")
        word_extractor = None
        noun_extractor = None

# ë¼ì´ë¸ŒëŸ¬ë¦¬ ì´ˆê¸°í™”
safe_import()

# êµ¬ê¸€ ì‹œíŠ¸ ìœ í‹¸ë¦¬í‹° ëª¨ë“ˆ
try:
    from google_sheets_utils_deploy import (
        load_data_from_google_sheets,
        preprocess_dataframe,
        test_connection,
        get_google_sheets_client,
        get_connection_status
    )
    SHEETS_UTILS_AVAILABLE = True
except ImportError:
    SHEETS_UTILS_AVAILABLE = False

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="ğŸš€ ë‰´ìŠ¤íŒ©í† ë¦¬",
    page_icon="ğŸš€",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ì „ì—­ ì„¤ì •
PERPLEXITY_API_KEY = st.secrets["PERPLEXITY_API_KEY"]
APP_PASSWORD = st.secrets["APP_PASSWORD"]


# ìºì‹œ ì„¤ì •
CACHE_DIR = Path("cache")
CACHE_DIR.mkdir(exist_ok=True)
DATA_CACHE_DIR = CACHE_DIR / "data"
DATA_CACHE_DIR.mkdir(exist_ok=True)
ANALYSIS_CACHE_DIR = CACHE_DIR / "analysis"
ANALYSIS_CACHE_DIR.mkdir(exist_ok=True)
API_CACHE_DIR = CACHE_DIR / "api_calls"
API_CACHE_DIR.mkdir(exist_ok=True)

# ì–´ë‘ìš´ í…Œë§ˆ CSS ìŠ¤íƒ€ì¼
st.markdown("""
<style>
.stApp {
    background-color: #0E1117;
    color: #FAFAFA;
}
.css-18e3th9 {
    background-color: #262730;
}
.css-1d391kg {
    background-color: #262730;
}
</style>
""", unsafe_allow_html=True)

# ================================
# ìºì‹± ì‹œìŠ¤í…œ
# ================================
def get_cache_key(data_type, industry, date_str=None):
    """ìºì‹œ í‚¤ ìƒì„±"""
    if date_str is None:
        date_str = datetime.now().strftime("%Y-%m-%d")
    key_string = f"{data_type}_{industry}_{date_str}"
    return hashlib.md5(key_string.encode()).hexdigest()

def save_to_cache(cache_type, key, data):
    """ìºì‹œì— ë°ì´í„° ì €ì¥"""
    try:
        if cache_type == "data":
            cache_file = DATA_CACHE_DIR / f"{key}.pkl"
        elif cache_type == "analysis":
            cache_file = ANALYSIS_CACHE_DIR / f"{key}.pkl"
        else:
            cache_file = API_CACHE_DIR / f"{key}.pkl"
        
        cache_data = {
            'data': data,
            'timestamp': time.time(),
            'date': datetime.now().strftime("%Y-%m-%d")
        }
        
        with open(cache_file, 'wb') as f:
            pickle.dump(cache_data, f)
        return True
    except Exception as e:
        st.error(f"ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {str(e)}")
        return False

def load_from_cache(cache_type, key, max_age_hours=24):
    """ìºì‹œì—ì„œ ë°ì´í„° ë¡œë“œ"""
    try:
        if cache_type == "data":
            cache_file = DATA_CACHE_DIR / f"{key}.pkl"
        elif cache_type == "analysis":
            cache_file = ANALYSIS_CACHE_DIR / f"{key}.pkl"
        else:
            cache_file = API_CACHE_DIR / f"{key}.pkl"
        
        if not cache_file.exists():
            return None
        
        with open(cache_file, 'rb') as f:
            cache_data = pickle.load(f)
        
        # ìºì‹œ ìœ íš¨ì„± ê²€ì‚¬
        cache_age = time.time() - cache_data['timestamp']
        if cache_age > max_age_hours * 3600:
            return None
        
        return cache_data['data']
    except:
        return None

# ================================
# API í˜¸ì¶œ
# ================================
def call_perplexity_api_cached(prompt, model="sonar-pro", max_age_hours=24):
    """ìºì‹±ëœ Perplexity API í˜¸ì¶œ"""
    try:
        # ìºì‹œ í‚¤ ìƒì„±
        cache_key = hashlib.md5(f"{prompt}_{model}".encode()).hexdigest()
        
        # ìºì‹œì—ì„œ ë¨¼ì € í™•ì¸
        cached_result = load_from_cache("api", cache_key, max_age_hours)
        if cached_result:
            st.info("ğŸ”„ ìºì‹œì—ì„œ AI ë¶„ì„ ê²°ê³¼ë¥¼ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.")
            return cached_result
        
        # API í˜¸ì¶œ
        headers = {
            "Authorization": f"Bearer {PERPLEXITY_API_KEY}",
            "Content-Type": "application/json"
        }
        
        data = {
            "model": model,
            "messages": [
                {
                    "role": "system",
                    "content": "ë‹¹ì‹ ì€ ë‰´ìŠ¤ ì „ë¬¸ ë¶„ì„ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ê¸°ì‚¬ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ í•µì‹¬ íŠ¸ë Œë“œì™€ ì¤‘ìš”í•œ í‚¤ì›Œë“œë¥¼ í•œêµ­ì–´ë¡œ ê°„ê²°í•˜ê²Œ ìš”ì•½í•´ì£¼ì„¸ìš”."
                },
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            "temperature": 0.3,
            "max_tokens": 1000
        }
        
        response = requests.post(
            "https://api.perplexity.ai/chat/completions",
            headers=headers,
            json=data,
            timeout=30
        )
        
        if response.status_code == 200:
            result = response.json()
            api_result = result["choices"][0]["message"]["content"]
            
            # ê²°ê³¼ë¥¼ ìºì‹œì— ì €ì¥
            save_to_cache("api", cache_key, api_result)
            st.success("ğŸ†• ìƒˆë¡œìš´ AI ë¶„ì„ì„ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤.")
            return api_result
        else:
            st.error(f"API ì˜¤ë¥˜: {response.status_code}")
            return None
    
    except Exception as e:
        st.error(f"API í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}")
        return None

# ================================
# ğŸ”§ NEW UTILS (ìƒˆë¡œìš´ ê¸°ëŠ¥ í•¨ìˆ˜ë“¤)
# ================================
def compile_articles_text(articles, max_each=3):
    """ìœ ì‚¬ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸ â†’ Promptìš© í…ìŠ¤íŠ¸ ë¬¸ìì—´"""
    texts = []
    for art in articles[:max_each]:
        title = art.get("ì œëª©", "")
        body = art.get("ì£¼ìš”ë‚´ìš©", "")
        if pd.notna(title):
            texts.append(f"ì œëª©: {title}\në‚´ìš©: {str(body)[:300]}")
    return "\n".join(texts)

def generate_monthly_ai_plans(insight_dict, df, industry="ì „ì²´"):
    """
    ì£¼ì°¨ë³„ í•µì‹¬ë¬¸ì¥ + ìœ ì‚¬ ê¸°ì‚¬ë“¤ì„ ë¬¶ì–´ Perplexity API ë¡œ 'AI ì¶”ì²œ ê¸°íš ì•„ì´í…œ' ìƒì„±
    """
    if not insight_dict or df.empty:
        return "ë°ì´í„°ê°€ ë¶€ì¡±í•´ ê¸°íš ì•„ì´í…œì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    plan_source_blocks = []
    for week, sentence in insight_dict.get("weekly_insights", {}).items():
        if not sentence:
            continue
        sims = find_similar_articles_to_insight(df, sentence, 7)
        sim_text = compile_articles_text([s["article"] for s in sims])
        plan_source_blocks.append(
            f"[{week}] í•µì‹¬ë¬¸ì¥: {sentence}\nê´€ë ¨ ê¸°ì‚¬:\n{sim_text}"
        )

    prompt = f"""
ë‹¹ì‹ ì€ í•œêµ­ ì£¼ìš” ì¼ê°„ì§€ì˜ {industry if industry!='ì „ì²´' else ''} ë‹´ë‹¹ ë¶€ì¥ì…ë‹ˆë‹¤.
ì•„ë˜ ì£¼ì°¨ë³„ í•µì‹¬ íŠ¸ë Œë“œì™€ ê´€ë ¨ ê¸°ì‚¬ë“¤ì„ ë¶„ì„í•´ ë‹¤ìŒ ë‹¬ ì·¨ì¬ ë°©í–¥ì„ ì œì‹œí• 
'AI ì¶”ì²œ ê¸°íš ì•„ì´í…œ' 5ê±´ì„ ì‘ì„±í•˜ì„¸ìš”.

[ìš”ì²­ í˜•ì‹]
ì œì‹œ ë¬¸ì¥(ê¸°ì‚¬ ì œëª© í›„ë³´) - í•œ ì¤„ ì´ìœ 

[ë¶„ì„ ëŒ€ìƒ ë°ì´í„°]
{os.linesep.join(plan_source_blocks[:3])}
"""
    return call_perplexity_api_cached(prompt, max_age_hours=6)

def generate_custom_topic_brief(df, industry, topic, weight_threshold=0.45):
    """
    ì—…ê³„+ì£¼ì œ ê´€ë ¨ ê¸°ì‚¬ ì¶”ì¶œ í›„ Perplexity APIë¡œ ê³ ê¸‰ ë¸Œë¦¬í•‘ ìƒì„±
    """
    if df.empty or not topic:
        return None

    # ì—…ê³„ í•„í„°
    pool = df if industry == "ì „ì²´" else df[df["ì—…ê³„"] == industry]
    if pool.empty:
        return None

    # ìœ ì‚¬ë„ ê³„ì‚°
    pool = pool.copy()
    pool["combined_text"] = pool["ì œëª©"].fillna("") + " " + pool["ì£¼ìš”ë‚´ìš©"].fillna("")
    
    if not TfidfVectorizer or not cosine_similarity:
        return "TF-IDF ë¶„ì„ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤."
    
    vec = TfidfVectorizer(max_features=1500)
    try:
        mat = vec.fit_transform(pool["combined_text"].tolist() + [topic])
        sims = cosine_similarity(mat[-1:], mat[:-1]).flatten()
        pool["sim"] = sims
        cand = pool[(pool["sim"] > 0.1) & (pool["ì „ì²´ê°€ì¤‘ì¹˜"] >= weight_threshold)]
        if cand.empty:
            return None

        cand = cand.sort_values(["sim", "ì „ì²´ê°€ì¤‘ì¹˜"], ascending=False).head(20)
        joined = [
            f"ì œëª©: {r['ì œëª©']}\në‚´ìš©: {str(r['ì£¼ìš”ë‚´ìš©'])[:300]}"
            for _, r in cand.iterrows()
        ]
        prompt = f"""
ë‹¹ì‹ ì€ í•œêµ­ ì–¸ë¡ ì‚¬ì˜ {industry if industry!='ì „ì²´' else ''} ë‹´ë‹¹ í¸ì§‘êµ­ì¥ì…ë‹ˆë‹¤.
ì£¼ì œ: {topic}

ì•„ë˜ ê¸°ì‚¬ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ
1) í˜„ì¬ í•µì‹¬ íŠ¸ë Œë“œ 3ê°€ì§€
2) ê° íŠ¸ë Œë“œë³„ ë§¤ì²´ ë³´ë„ íŠ¹ì§•
3) ì•ìœ¼ë¡œ ë°œêµ´í•´ì•¼ í•  ì‹¬ì¸µ ê¸°íš ê¸°ì‚¬ 3ê±´(ì œëª©+ê°„ë‹¨ ì´ìœ )
ì„ ê¸°ìë“¤ì—ê²Œ ì§€ì‹œí•˜ëŠ” êµ¬ì–´ì²´ ë©”ëª¨ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.

ê¸°ì‚¬ ëª©ë¡:
{os.linesep.join(joined[:10])}
"""
        return call_perplexity_api_cached(prompt, max_age_hours=6)
    except:
        return "ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

# ================================
# ìƒˆë¡œìš´ AI ê¸°ëŠ¥ í•¨ìˆ˜ë“¤
# ================================
def generate_today_planning_items(df, industry="ì „ì²´"):
    """
    ìƒìœ„ ê°€ì¤‘ì¹˜ ê¸°ì‚¬ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ Perplexity APIë¥¼ í˜¸ì¶œ,
    'ì˜¤ëŠ˜ì˜ ê¸°íš ì•„ì´í…œ' 3ê°œë¥¼ ì œì•ˆí•œë‹¤.
    """
    if df.empty:
        return "ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."

    if industry != "ì „ì²´":
        df = df[df["ì—…ê³„"] == industry]

    # ê°€ì¤‘ì¹˜ ìƒìœ„ ê¸°ì‚¬ 20ê°œê¹Œì§€ ëŒ€ìƒ
    top_articles = (
        df.nlargest(20, "ì „ì²´ê°€ì¤‘ì¹˜") if "ì „ì²´ê°€ì¤‘ì¹˜" in df.columns else df.head(20)
    )

    texts = []
    for _, art in top_articles.iterrows():
        title = art.get("ì œëª©", "")
        body = art.get("ì£¼ìš”ë‚´ìš©", "")
        if pd.notna(title) and pd.notna(body):
            texts.append(f"ì œëª©: {title}\në‚´ìš©: {body}")
        elif pd.notna(title):
            texts.append(f"ì œëª©: {title}")

    if not texts:
        return "ë¶„ì„í•  ê¸°ì‚¬ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤."

    prompt = f"""
ë‹¤ìŒì€ ì˜¤ëŠ˜ì˜ ì£¼ìš” ë‰´ìŠ¤ ê¸°ì‚¬ë“¤ì…ë‹ˆë‹¤. ê¸°ìë“¤ì´ ì¶”ê°€ ì·¨ì¬í•˜ê¸° ì¢‹ì€ 'ê¸°íš ì•„ì´í…œ' 3ê±´ì„ ì œì•ˆí•´ì£¼ì„¸ìš”.

ê¸°ì‚¬ ëª©ë¡:
{os.linesep.join(texts[:15])}

[ë‹µë³€ í˜•ì‹]
1. ê¸°íš ì•„ì´í…œ ì œëª©:
   - ì·¨ì¬ í¬ì¸íŠ¸:
   - ì˜ˆìƒ ì†ŒìŠ¤:

2. ê¸°íš ì•„ì´í…œ ì œëª©:
   - ì·¨ì¬ í¬ì¸íŠ¸:
   - ì˜ˆìƒ ì†ŒìŠ¤:

3. ê¸°íš ì•„ì´í…œ ì œëª©:
   - ì·¨ì¬ í¬ì¸íŠ¸:
   - ì˜ˆìƒ ì†ŒìŠ¤:
"""
    return (
        call_perplexity_api_cached(prompt, max_age_hours=6)
        or "ê¸°íš ì•„ì´í…œ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
    )

def generate_monthly_weekly_insights(df, target_month=None):
    """
    ì›”ë³„Â·ì£¼ì°¨ë³„ í•µì‹¬ ë¬¸ì¥ ìƒì„±(PPLX API) â†’ dict ë°˜í™˜
    { 'monthly_insight': str,
      'weekly_insights': {ì£¼ì°¨: str, â€¦} }
    """
    if df.empty:
        return {}

    if target_month and "ì›”" in df.columns:
        df = df[df["ì›”"] == target_month]

    if df.empty:
        return {}

    # ì›” ì „ì²´ í•µì‹¬ ë¬¸ì¥
    month_txt = " ".join(
        (
            f"{r.get('ì œëª©','')} {r.get('ì£¼ìš”ë‚´ìš©','')}"
            for _, r in df.head(50).iterrows()
        )
    )
    mon_prompt = f"""
ë‹¤ìŒì€ {target_month or 'ì´ë²ˆ ë‹¬'} ì£¼ìš” ë‰´ìŠ¤ì…ë‹ˆë‹¤.
í•µì‹¬ ì´ìŠˆë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ ì£¼ì„¸ìš”.
ë‰´ìŠ¤ ë‚´ìš©:
{month_txt}
"""
    monthly_insight = call_perplexity_api_cached(mon_prompt, max_age_hours=12)

    # ì£¼ì°¨ë³„
    weekly_insights = {}
    if "ì£¼ì°¨" in df.columns:
        for week in sorted(df["ì£¼ì°¨"].dropna().unique()):
            w_df = df[df["ì£¼ì°¨"] == week]
            week_txt = " ".join(
                (
                    f"{r.get('ì œëª©','')} {r.get('ì£¼ìš”ë‚´ìš©','')}"
                    for _, r in w_df.head(20).iterrows()
                )
            )
            w_prompt = f"""
ë‹¤ìŒì€ {week} ì£¼ìš” ë‰´ìŠ¤ì…ë‹ˆë‹¤.
í•µì‹¬ ì´ìŠˆë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ ì£¼ì„¸ìš”.
ë‰´ìŠ¤ ë‚´ìš©:
{week_txt}
"""
            weekly_insights[week] = call_perplexity_api_cached(
                w_prompt, max_age_hours=24
            )

    return {"monthly_insight": monthly_insight, "weekly_insights": weekly_insights}

def find_similar_articles_to_insight(df, insight, top_n=5):
    """
    í•µì‹¬ ë¬¸ì¥(insight) ê³¼ ìœ ì‚¬ë„ê°€ ë†’ì€ ê¸°ì‚¬ nê°œ ë°˜í™˜
    """
    if df.empty or not insight:
        return []

    df = df.copy()
    df["combined_text"] = df.apply(
        lambda r: f"{r.get('ì œëª©','')} {r.get('ì£¼ìš”ë‚´ìš©','')}", axis=1
    )
    df = df[df["combined_text"].str.strip() != ""]

    if df.empty or not (TfidfVectorizer and cosine_similarity):
        return []

    try:
        vec = TfidfVectorizer(max_features=1000)
        mat = vec.fit_transform(df["combined_text"].tolist() + [insight])
        sims = cosine_similarity(mat[-1:], mat[:-1]).flatten()
        idxs = sims.argsort()[::-1][:top_n]

        return [
            {"article": df.iloc[i], "similarity": sims[i]}
            for i in idxs
            if sims[i] > 0.1
        ]
    except:
        return []

# ================================
# soynlp ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ
# ================================
def extract_keywords_soynlp(text_list, top_n=15):
    """soynlpë¥¼ ì‚¬ìš©í•œ ê³ ê¸‰ í‚¤ì›Œë“œ ì¶”ì¶œ"""
    try:
        if not text_list or not word_extractor:
            return extract_keywords_simple(text_list, top_n)
        
        # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
        if isinstance(text_list, str):
            text_list = [text_list]
        
        # í…ìŠ¤íŠ¸ ì •ì œ
        cleaned_texts = []
        for text in text_list:
            if pd.notna(text):
                # ê¸°ë³¸ ì •ì œ
                text = str(text).strip()
                text = re.sub(r'[^\w\sê°€-í£]', ' ', text)
                text = re.sub(r'\s+', ' ', text)
                if len(text) > 10:  # ë„ˆë¬´ ì§§ì€ í…ìŠ¤íŠ¸ ì œì™¸
                    cleaned_texts.append(text)
        
        if not cleaned_texts:
            return []
        
        # soynlp í•™ìŠµ ë° ì¶”ì¶œ
        corpus = ' '.join(cleaned_texts)
        
        # ë‹¨ì–´ ì¶”ì¶œ
        word_extractor.train([corpus])
        words = word_extractor.extract()
        
        # ëª…ì‚¬ ì¶”ì¶œ
        noun_extractor.train(cleaned_texts)
        nouns = noun_extractor.extract()
        
        # í‚¤ì›Œë“œ ì ìˆ˜ ê³„ì‚°
        keyword_scores = {}
        
        # ì¶”ì¶œëœ ë‹¨ì–´ ì ìˆ˜ ê³„ì‚°
        for word, score in words.items():
            if (len(word) >= 2 and
                score.cohesion_forward >= 0.08 and
                score.right_branching_entropy >= 0.5):
                keyword_scores[word] = score.cohesion_forward * score.right_branching_entropy
        
        # ì¶”ì¶œëœ ëª…ì‚¬ ì ìˆ˜ ê³„ì‚°
        for noun, score in nouns.items():
            if len(noun) >= 2 and score.score >= 0.3:
                if noun in keyword_scores:
                    keyword_scores[noun] += score.score * 0.5
                else:
                    keyword_scores[noun] = score.score
        
        # ë¶ˆìš©ì–´ ì œê±°
        stopwords = {
            'ê¸°ì', 'ì‚¬ì§„', 'ì œê³µ', 'ê´€ë ¨', 'ì—…ê³„', 'ì‹œì¥', 'ë¶„ì•¼', 'íšŒì‚¬', 'ê¸°ì—…',
            'ë°œí‘œ', 'ê³µê°œ', 'ì„¤ëª…', 'ë³´ë„', 'ë‰´ìŠ¤', 'ê¸°ì‚¬', 'ì˜¤ëŠ˜', 'ì–´ì œ', 'ë‚´ì¼',
            'ìµœê·¼', 'í˜„ì¬', 'ë‹¹ì‹œ', 'ì´ë²ˆ', 'ë‹¤ìŒ', 'ì§€ë‚œ', 'êµ­ë‚´', 'í•´ì™¸', 'ì „êµ­',
            'ê²ƒ', 'ìˆ˜', 'ë“±', 'ë°', 'ë˜', 'ë”', 'ì´', 'ê·¸', 'ì„', 'ë¥¼', 'ì˜', 'ê°€',
            'ë§í–ˆë‹¤', 'ë°í˜”ë‹¤', 'ì „í–ˆë‹¤', 'í–ˆë‹¤', 'ìˆë‹¤', 'ì—†ë‹¤', 'í•œë‹¤', 'ëœë‹¤'
        }
        
        # ìµœì¢… í‚¤ì›Œë“œ ì„ ë³„
        final_keywords = []
        for word, score in sorted(keyword_scores.items(), key=lambda x: x[1], reverse=True):
            if (word not in stopwords and
                not word.isdigit() and
                len(word) <= 10 and
                re.search(r'[ê°€-í£]', word)):
                final_keywords.append((word, round(score, 3)))
        
        return final_keywords[:top_n]
    
    except Exception as e:
        st.error(f"soynlp í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
        # ë°±ì—… ë°©ë²•
        return extract_keywords_simple(text_list, top_n)

def extract_keywords_simple(text_list, top_n=10):
    """ë°±ì—…ìš© ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
    try:
        if isinstance(text_list, str):
            text_list = [text_list]
        
        all_text = ' '.join([str(text) for text in text_list if pd.notna(text)])
        
        # ê°„ë‹¨í•œ í† í°í™”
        words = re.findall(r'[ê°€-í£]{2,6}', all_text)
        
        # ë¶ˆìš©ì–´ ì œê±°
        stopwords = {
            'ê¸°ì', 'ì‚¬ì§„', 'ì œê³µ', 'ê´€ë ¨', 'ì—…ê³„', 'ì‹œì¥', 'ë¶„ì•¼', 'íšŒì‚¬', 'ê¸°ì—…',
            'ë°œí‘œ', 'ê³µê°œ', 'ì„¤ëª…', 'ë³´ë„', 'ë‰´ìŠ¤', 'ê¸°ì‚¬', 'ì˜¤ëŠ˜', 'ì–´ì œ', 'ë‚´ì¼'
        }
        
        keywords = [word for word in words if word not in stopwords]
        word_freq = Counter(keywords)
        return word_freq.most_common(top_n)
    except:
        return []

# ê¸°ì¡´ extract_keywords_mecab í•¨ìˆ˜ ëŒ€ì²´
def extract_keywords_mecab(text, top_n=10):
    """í‚¤ì›Œë“œ ì¶”ì¶œ - soynlp ìš°ì„  ì‚¬ìš©"""
    if isinstance(text, str):
        return extract_keywords_soynlp([text], top_n)
    elif isinstance(text, list):
        return extract_keywords_soynlp(text, top_n)
    else:
        return extract_keywords_soynlp([str(text)], top_n)

# ================================
# ì—…ê³„ë³„ í‚¤ì›Œë“œ ì¶”ì¶œ ê¸°ëŠ¥
# ================================
def extract_industry_keywords(df, industry):
    """ì—…ê³„ë³„ íŠ¹í™” í‚¤ì›Œë“œ ì¶”ì¶œ"""
    if industry != 'ì „ì²´':
        industry_articles = df[df['ì—…ê³„'] == industry]
    else:
        industry_articles = df
    
    titles = industry_articles['ì œëª©'].dropna().tolist()
    
    # soynlpë¡œ ì—…ê³„ íŠ¹í™” í‚¤ì›Œë“œ ì¶”ì¶œ
    keywords = extract_keywords_soynlp(titles, 20)
    
    # ì—…ê³„ë³„ ê°€ì¤‘ì¹˜ ì¡°ì •
    industry_weights = {
        'ìë™ì°¨': ['ì „ê¸°ì°¨', 'ììœ¨ì£¼í–‰', 'ë°°í„°ë¦¬', 'ëª¨ë¹Œë¦¬í‹°', 'ì „ë™í™”'],
        'IT/ì „ì': ['AI', 'ë°˜ë„ì²´', 'í´ë¼ìš°ë“œ', 'ë””ì§€í„¸', 'ì¸ê³µì§€ëŠ¥'],
        'ê¸ˆìœµ': ['í•€í…Œí¬', 'ê°€ìƒí™”í', 'ë¸”ë¡ì²´ì¸', 'íˆ¬ì', 'ë””ì§€í„¸ë±…í‚¹'],
        'ë°”ì´ì˜¤': ['ë°±ì‹ ', 'ì¹˜ë£Œì œ', 'ì„ìƒ', 'ì‹ ì•½', 'ì˜ë£Œ'],
        'í™”í•™/ì—ë„ˆì§€': ['ì¹œí™˜ê²½', 'íƒ„ì†Œì¤‘ë¦½', 'ì¬ìƒì—ë„ˆì§€', 'ìˆ˜ì†Œ'],
        'í•­ê³µ/ìš´ì†¡': ['ë¬¼ë¥˜', 'ë°°ì†¡', 'ëª¨ë¹Œë¦¬í‹°', 'ìš´ì†¡'],
        'ê±´ì„¤/ë¶€ë™ì‚°': ['ê°œë°œ', 'ë¶„ì–‘', 'ì¬ê±´ì¶•', 'ë¦¬ëª¨ë¸ë§'],
        'ìœ í†µ/ì†Œë¹„ì¬': ['ì´ì»¤ë¨¸ìŠ¤', 'ì˜¨ë¼ì¸', 'ë°°ì†¡', 'ì†Œë¹„'],
        'ì—”í„°í…Œì¸ë¨¼íŠ¸/ë¯¸ë””ì–´': ['ì½˜í…ì¸ ', 'í”Œë«í¼', 'ìŠ¤íŠ¸ë¦¬ë°', 'OTT'],
        'ê²Œì„': ['ë©”íƒ€ë²„ìŠ¤', 'ê²Œì„', 'í”Œë«í¼', 'ëª¨ë°”ì¼'],
        'ì‹í’ˆ/ìŒë£Œ': ['ê±´ê°•', 'í”„ë¦¬ë¯¸ì—„', 'ì¹œí™˜ê²½', 'ìœ ê¸°ë†']
    }
    
    if industry in industry_weights:
        boost_keywords = industry_weights[industry]
        enhanced_keywords = []
        for word, score in keywords:
            if any(boost in word for boost in boost_keywords):
                enhanced_keywords.append((word, score * 1.5))
            else:
                enhanced_keywords.append((word, score))
        return sorted(enhanced_keywords, key=lambda x: x[1], reverse=True)
    
    return keywords

# ================================
# ë„¤ì´ë²„ ë§í¬ í•˜ì´í¼ë§í¬ ì²˜ë¦¬ í•¨ìˆ˜ (í•µì‹¬ ìˆ˜ì • ë¶€ë¶„)
# ================================
def get_naver_link_column(df):
    """êµ¬ê¸€ ì‹œíŠ¸ì—ì„œ ë„¤ì´ë²„ ë§í¬ ì»¬ëŸ¼ëª…ì„ ì°¾ëŠ” í•¨ìˆ˜"""
    possible_columns = ['ë„¤ì´ë²„ë§í¬', 'ë„¤ì´ë²„ ë§í¬', 'ë„¤ì´ë²„URL', 'ë„¤ì´ë²„ URL', 'naver_link', 'naver_url']
    
    for col in possible_columns:
        if col in df.columns:
            return col
    
    # ìœ ì‚¬í•œ ì»¬ëŸ¼ëª… ì°¾ê¸°
    for col in df.columns:
        if 'ë„¤ì´ë²„' in str(col) and ('ë§í¬' in str(col) or 'URL' in str(col)):
            return col
    
    return None

def get_media_link_column(df):
    """êµ¬ê¸€ ì‹œíŠ¸ì—ì„œ ë§¤ì²´ë§í¬ ì»¬ëŸ¼ëª…ì„ ì°¾ëŠ” í•¨ìˆ˜"""
    possible_columns = ['ë§¤ì²´ë§í¬', 'ë§¤ì²´ ë§í¬', 'ë§¤ì²´URL', 'ë§¤ì²´ URL', 'media_link', 'media_url']
    
    for col in possible_columns:
        if col in df.columns:
            return col
    
    # ìœ ì‚¬í•œ ì»¬ëŸ¼ëª… ì°¾ê¸°
    for col in df.columns:
        if 'ë§¤ì²´' in str(col) and ('ë§í¬' in str(col) or 'URL' in str(col)):
            return col
    
    return None

def create_article_hyperlink(title, naver_url=None, media_url=None):
    """ê¸°ì‚¬ ì œëª©ì— í•˜ì´í¼ë§í¬ë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜"""
    if pd.isna(title):
        title = "ì œëª© ì—†ìŒ"
    else:
        title = str(title).strip()
    
    # 1ìˆœìœ„: ë„¤ì´ë²„ë§í¬ ì‚¬ìš©
    if pd.notna(naver_url) and str(naver_url).strip() and str(naver_url).strip().lower() not in ['nan', '', 'none']:
        naver_url = str(naver_url).strip()
        if not naver_url.startswith(('http://', 'https://')):
            naver_url = 'https://' + naver_url
        return f'<a href="{naver_url}" target="_blank">ğŸ“° {title}</a>'
    
    # 2ìˆœìœ„: ë§¤ì²´ë§í¬ ì‚¬ìš©
    if pd.notna(media_url) and str(media_url).strip() and str(media_url).strip().lower() not in ['nan', '', 'none']:
        media_url = str(media_url).strip()
        if not media_url.startswith(('http://', 'https://')):
            media_url = 'https://' + media_url
        return f'<a href="{media_url}" target="_blank">ğŸ“° {title}</a>'
    
    return f"ğŸ“„ {title}"

def display_article_with_link(article, df=None):
    """ê¸°ì‚¬ë¥¼ í•˜ì´í¼ë§í¬ì™€ í•¨ê»˜ í‘œì‹œí•˜ëŠ” í•¨ìˆ˜"""
    title = article.get('ì œëª©', 'ì œëª© ì—†ìŒ')
    
    # ë„¤ì´ë²„ ë§í¬ ì»¬ëŸ¼ëª… ìë™ ê°ì§€
    if df is not None:
        naver_col = get_naver_link_column(df)
        naver_url = article.get(naver_col) if naver_col else None
        
        # ë§¤ì²´ ë§í¬ ì»¬ëŸ¼ëª… ìë™ ê°ì§€
        media_col = get_media_link_column(df)
        media_url = article.get(media_col) if media_col else None
    else:
        naver_url = article.get('ë„¤ì´ë²„ë§í¬') or article.get('ë„¤ì´ë²„ URL')
        media_url = article.get('ë§¤ì²´ë§í¬') or article.get('ë§¤ì²´ URL')
    
    # í•˜ì´í¼ë§í¬ê°€ ì ìš©ëœ ì œëª© ìƒì„±
    linked_title = create_article_hyperlink(title, naver_url, media_url)
    return linked_title

# ================================
# PyArrow ì˜¤ë¥˜ í•´ê²°ìš© ì•ˆì „í•œ í…Œì´ë¸” í‘œì‹œ í•¨ìˆ˜
# ================================
def safe_display_dataframe(df, title="ë°ì´í„°"):
    """PyArrow ì˜¤ë¥˜ë¥¼ í”¼í•œ ì•ˆì „í•œ ë°ì´í„°í”„ë ˆì„ í‘œì‹œ"""
    try:
        # ë¨¼ì € st.dataframe ì‹œë„
        st.dataframe(df, use_container_width=True)
    except Exception as e:
        # PyArrow ì˜¤ë¥˜ ì‹œ ëŒ€ì•ˆ í‘œì‹œ
        st.warning(f"âš ï¸ í‘œ í‘œì‹œ ì˜¤ë¥˜ (PyArrow): {str(e)}")
        st.markdown(f"### ğŸ“Š {title}")
        
        # HTML í…Œì´ë¸”ë¡œ ëŒ€ì²´
        html_table = "<table style='width:100%; border-collapse: collapse;'>"
        html_table += "<tr style='background-color: #333; color: white;'>"
        for col in df.columns:
            html_table += f"<th style='border: 1px solid #ddd; padding: 8px;'>{col}</th>"
        html_table += "</tr>"
        
        for _, row in df.head(10).iterrows():  # ìµœëŒ€ 10ê°œë§Œ í‘œì‹œ
            html_table += "<tr>"
            for col in df.columns:
                html_table += f"<td style='border: 1px solid #ddd; padding: 8px;'>{row[col]}</td>"
            html_table += "</tr>"
        html_table += "</table>"
        html_table += f"<p><em>... ì´ {len(df)}ê°œ í–‰ ì¤‘ 10ê°œë§Œ í‘œì‹œ</em></p>"
        
        st.markdown(html_table, unsafe_allow_html=True)

# ================================
# ë¡œê·¸ì¸ í•¨ìˆ˜
# ================================
def check_password():
    """ê¸°ììš© ë¡œê·¸ì¸ í™”ë©´"""
    if st.session_state.get('authenticated', False):
        return True
    
    st.markdown("""
    <div style='text-align: center; padding: 2rem; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); 
                border-radius: 10px; margin-bottom: 2rem;'>
        <h1 style='color: white; margin-bottom: 1rem;'>ğŸš€ ë‰´ìŠ¤íŒ©í† ë¦¬</h1>
        <p style='color: white; font-size: 1.1rem; margin-bottom: 0;'>
            ê¸°ìë‹˜ë“¤ì„ ìœ„í•œ ì „ìš© ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤.
        </p>
    </div>
    """, unsafe_allow_html=True)
    
    col1, col2, col3 = st.columns([1, 2, 1])
    
    with col2:
        st.markdown("### ğŸ” ë¡œê·¸ì¸")
        password = st.text_input("ë¹„ë°€ë²ˆí˜¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”:", type="password", key="password_input")
        
        col_a, col_b, col_c = st.columns([1, 1, 1])
        with col_b:
            if st.button("ğŸšª ë¡œê·¸ì¸", use_container_width=True):
                if password == APP_PASSWORD:
                    st.session_state['authenticated'] = True
                    st.success("âœ… ë¡œê·¸ì¸ ì„±ê³µ!")
                    st.rerun()
                else:
                    st.error("âŒ ë¹„ë°€ë²ˆí˜¸ê°€ ì˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    return False

# ================================
# ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜
# ================================
def main():
    """ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜ í•¨ìˆ˜"""
    
    # ë¡œê·¸ì¸ í™•ì¸
    if not check_password():
        return
    
    # ì‚¬ì´ë“œë°” - ë¡œê·¸ì•„ì›ƒ ë²„íŠ¼
    with st.sidebar:
        st.markdown("---")
        if st.button("ğŸšª ë¡œê·¸ì•„ì›ƒ", type="secondary"):
            st.session_state['authenticated'] = False
            st.rerun()
    
    # í—¤ë”
    st.markdown("""
    <div style='text-align: center; padding: 1rem; background: linear-gradient(90deg, #FF6B6B, #4ECDC4); 
                border-radius: 10px; margin-bottom: 2rem;'>
        <h1 style='color: white; margin: 0;'>ğŸš€ ë‰´ìŠ¤íŒ©í† ë¦¬</h1>
        <p style='color: white; margin: 0; font-size: 1.1rem;'>AI ê¸°ë°˜ ê¸°ì‚¬ ë¶„ì„ í”Œë«í¼</p>
    </div>
    """, unsafe_allow_html=True)
    
    # ë°ì´í„° ë¡œë“œ
    @st.cache_data(ttl=1800)  # 30ë¶„ ìºì‹œ
    def load_and_process_data():
        if SHEETS_UTILS_AVAILABLE:
            with st.spinner("ğŸ“Š êµ¬ê¸€ ì‹œíŠ¸ì—ì„œ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ëŠ” ì¤‘..."):
                df = load_data_from_google_sheets()
                if not df.empty:
                    df = preprocess_dataframe(df)
                    st.success(f"âœ… {len(df)}ê°œ ê¸°ì‚¬ ë°ì´í„° ë¡œë“œ ì™„ë£Œ!")
                    return df
                else:
                    st.error("âŒ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    return pd.DataFrame()
        else:
            st.error("âŒ êµ¬ê¸€ ì‹œíŠ¸ ì—°ê²° ëª¨ë“ˆì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return pd.DataFrame()
    
    df = load_and_process_data()
    
    # ì‚¬ì´ë“œë°” - ì—…ê³„ ì„ íƒ
    with st.sidebar:
        st.markdown("## ğŸ¯ í•„í„° ì„¤ì •")
        
        if not df.empty and 'ì—…ê³„' in df.columns:
            industries = ["ì „ì²´"] + sorted(df['ì—…ê³„'].dropna().unique().tolist())
            selected_industry = st.selectbox(
                "ğŸ“Š ì—…ê³„ ì„ íƒ",
                industries,
                index=0,
                key="industry_selector"
            )
        else:
            selected_industry = "ì „ì²´"
        
        # ì—°ê²° ìƒíƒœ í‘œì‹œ
        if SHEETS_UTILS_AVAILABLE:
            status = get_connection_status()
            if status['connected']:
                st.success("âœ… êµ¬ê¸€ ì‹œíŠ¸ ì—°ê²°ë¨")
            else:
                st.error(f"âŒ ì—°ê²° ì‹¤íŒ¨: {status['message']}")
        
        # ë°ì´í„° í†µê³„
        if not df.empty:
            st.markdown("### ğŸ“ˆ ë°ì´í„° í†µê³„")
            st.metric("ì´ ê¸°ì‚¬ ìˆ˜", len(df))
            if 'ì—…ê³„' in df.columns:
                st.metric("ì—…ê³„ ìˆ˜", df['ì—…ê³„'].nunique())
            if 'ë§¤ì²´' in df.columns:
                st.metric("ë§¤ì²´ ìˆ˜", df['ë§¤ì²´'].nunique())
    
    # ì—…ê³„ë³„ í•„í„°ë§
    if selected_industry != "ì „ì²´" and not df.empty:
        filtered_df = df[df['ì—…ê³„'] == selected_industry]
    else:
        filtered_df = df
    
    # ë©”ì¸ íƒ­ (íŠ¸ë Œë“œ ë¶„ì„ ì œê±° - 3ê°œ íƒ­)
    tab1, tab2, tab3 = st.tabs(
        ["ğŸ“Š ì˜¤ëŠ˜ì˜ ë¦¬ë·°", "ğŸ—“ï¸ ì›”Â·ì£¼ì°¨ ë¦¬ë·°", "ğŸ¯ ë§ì¶¤ ë¶„ì„"]
    )
    
    # ---------- ğŸ“Š ì˜¤ëŠ˜ì˜ ë¦¬ë·° ----------
    with tab1:
        st.header("ğŸ“Š ì˜¤ëŠ˜ì˜ ë¦¬ë·°")
        
        if filtered_df.empty:
            st.info("ì„ íƒëœ ì—…ê³„ì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        else:
            st.subheader("ğŸ”¥ ì£¼ìš” ë³´ë„")
            
            # ìƒìœ„ 10ê°œ ê¸°ì‚¬ í‘œì‹œ (í•˜ì´í¼ë§í¬ ì ìš©)
            top_articles = (
                filtered_df.nlargest(10, "ì „ì²´ê°€ì¤‘ì¹˜") 
                if "ì „ì²´ê°€ì¤‘ì¹˜" in filtered_df.columns 
                else filtered_df.head(10)
            )
            
            for _, art in top_articles.iterrows():
                linked_title = display_article_with_link(art, filtered_df)
                
                col1, col2 = st.columns([3, 1])
                
                with col1:
                    st.markdown(f"**{linked_title}**", unsafe_allow_html=True)
                    
                    # ë©”íƒ€ ì •ë³´
                    meta = []
                    if pd.notna(art.get("ë§¤ì²´")):
                        meta.append(f"ğŸ“º {art.get('ë§¤ì²´')}")
                    if pd.notna(art.get("ë³´ë„ë‚ ì§œ")):
                        meta.append(f"ğŸ“… {art.get('ë³´ë„ë‚ ì§œ')}")
                    if meta:
                        st.markdown(" | ".join(meta))
                    
                    # ì£¼ìš” ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°
                    if pd.notna(art.get("ì£¼ìš”ë‚´ìš©")):
                        st.markdown(f"*{str(art.get('ì£¼ìš”ë‚´ìš©'))[:200]}â€¦*")
                
                with col2:
                    if "ì „ì²´ê°€ì¤‘ì¹˜" in art:
                        st.metric("ê°€ì¤‘ì¹˜", f"{art['ì „ì²´ê°€ì¤‘ì¹˜']:.2f}")
            
            st.divider()
            
            # ğŸ”˜ AI ì¶”ì²œ ë²„íŠ¼
            if st.button("ğŸ§  AI ì¶”ì²œ", key="ai_planning_btn"):
                with st.spinner("AIê°€ ê¸°íš ì•„ì´í…œì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤â€¦"):
                    plan_text = generate_today_planning_items(filtered_df, selected_industry)
                st.markdown("### ğŸ“Œ ì˜¤ëŠ˜ì˜ ê¸°íš ì•„ì´í…œ")
                st.markdown(plan_text)
    
    # ---------- ğŸ—“ï¸ ì›”Â·ì£¼ì°¨ ë¦¬ë·° ----------
    with tab2:
        st.header("ğŸ—“ï¸ ì›”Â·ì£¼ì°¨ë³„ í•µì‹¬ ì´ìŠˆ")
        
        if filtered_df.empty:
            st.info("ì„ íƒëœ ì—…ê³„ì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        else:
            # ì›” ì„ íƒ
            if "ì›”" in filtered_df.columns:
                months = sorted(filtered_df["ì›”"].dropna().unique())
                target_month = st.selectbox(
                    "ë¶„ì„í•  ì›” ì„ íƒ", 
                    months, 
                    key="month_sel"
                )
                
                if st.button("ğŸ¤– í•µì‹¬ë¬¸ì¥ ë¶„ì„", key="ai_month_btn"):
                    with st.spinner("AIê°€ ì›”Â·ì£¼ì°¨ í•µì‹¬ë¬¸ì¥ì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤â€¦"):
                        insight_dict = generate_monthly_weekly_insights(
                            filtered_df, target_month
                        )
                    st.session_state["insight_dict"] = insight_dict  # ì €ì¥

                insight_dict = st.session_state.get("insight_dict", {})
                if insight_dict:
                    # ì›” í•µì‹¬ë¬¸ì¥
                    if insight_dict.get("monthly_insight"):
                        st.subheader(f"ğŸ“… {target_month} í•µì‹¬ ë¬¸ì¥")
                        st.success(insight_dict["monthly_insight"])

                    # ì£¼ì°¨ë³„ (í•˜ì´í¼ë§í¬ ì ìš©)
                    for week, sent in insight_dict.get("weekly_insights", {}).items():
                        if not sent:
                            continue
                        st.markdown(f"#### {week} í•µì‹¬ ë¬¸ì¥")
                        st.info(sent)

                        # ìœ ì‚¬ ê¸°ì‚¬ ì¶”ì¶œ (í•˜ì´í¼ë§í¬ ì ìš©)
                        sims = find_similar_articles_to_insight(filtered_df, sent, 5)
                        for s in sims:
                            art = s["article"]
                            link = display_article_with_link(art, filtered_df)
                            st.markdown(
                                f"- {link} (ìœ ì‚¬ë„ {s['similarity']:.2f})",
                                unsafe_allow_html=True,
                            )
                    st.divider()

                    # ğŸ§  AI ì¶”ì²œ ê¸°íš ì•„ì´í…œ ë²„íŠ¼ (ìƒˆë¡œìš´ ê¸°ëŠ¥)
                    if st.button("ğŸ§  AI ì¶”ì²œ ê¸°íš ì•„ì´í…œ", key="ai_month_plan_btn"):
                        with st.spinner("AIê°€ ê¸°íš ì•„ì´í…œì„ ì œì•ˆ ì¤‘ì…ë‹ˆë‹¤â€¦"):
                            plan_text = generate_monthly_ai_plans(
                                insight_dict, filtered_df, selected_industry
                            )
                        st.markdown("### ğŸ“ AI ì¶”ì²œ ê¸°íš ì•„ì´í…œ")
                        st.markdown(plan_text or "ìƒì„± ì‹¤íŒ¨")
            else:
                st.info("ë‚ ì§œ ì •ë³´ê°€ ì—†ì–´ ì›”ë³„ ë¶„ì„ì´ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤.")
    
    # ---------- ğŸ¯ ë§ì¶¤ ë¶„ì„ (ìƒˆë¡œìš´ ê¸°ëŠ¥ ì ìš©) ----------
    with tab3:
        st.header("ğŸ¯ ë§ì¶¤ ë¶„ì„")
        
        if filtered_df.empty:
            st.info("ì„ íƒëœ ì—…ê³„ì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        else:
            st.subheader("ğŸ” ì—…ê³„Â·ì£¼ì œ ì…ë ¥")
            
            # ì—…ê³„ ì„ íƒ
            industry_opt = ["ì „ì²´"] + sorted(df["ì—…ê³„"].dropna().unique())
            chosen_industry = st.selectbox("ì—…ê³„ ì„ íƒ", industry_opt, key="custom_ind")
            
            # ì£¼ì œ ì…ë ¥
            topic_query = st.text_input(
                "ì£¼ì œ ì…ë ¥ (ì˜ˆ: ì „ê¸°ì°¨ ë°°í„°ë¦¬ ì›ê°€)", 
                key="custom_topic"
            )

            if st.button("ğŸ¤– AI ê³ ê¸‰ ë¶„ì„", key="custom_topic_btn"):
                with st.spinner("AIê°€ ë§ì¶¤ ë¶„ì„ì„ ì¤€ë¹„ ì¤‘ì…ë‹ˆë‹¤â€¦"):
                    brief = generate_custom_topic_brief(
                        df, chosen_industry, topic_query, weight_threshold=0.45
                    )
                if brief:
                    st.markdown("### ğŸ§  AI í¸ì§‘êµ­ì¥ ë¶„ì„ ë©”ëª¨")
                    st.markdown(brief)
                else:
                    st.warning("ë¶„ì„ì— ì í•©í•œ ê¸°ì‚¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
    
    # í‘¸í„°
    st.markdown("---")
    st.markdown("""
    <div style='text-align: center; color: #666; padding: 1rem;'>
        ğŸš€ ë‰´ìŠ¤íŒ©í† ë¦¬ v3.2.0 | AI ê¸°ë°˜ ê¸°ì‚¬ ë¶„ì„ í”Œë«í¼<br>
        Made with â¤ï¸ for ê¸°ìë‹˜ë“¤
    </div>
    """, unsafe_allow_html=True)

# ì‹¤í–‰
if __name__ == "__main__":
    main()
