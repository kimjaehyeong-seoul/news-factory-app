# ğŸš€ ë‰´ìŠ¤íŒ©í† ë¦¬ - AI ê¸°ë°˜ ê¸°ì‚¬ ë¶„ì„ í”Œë«í¼ (konlpy ë²„ì „ + ë°±ì—…íŒŒì¼ ê¸°ëŠ¥ ë³µì›)
import streamlit as st
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import re
import os
import time
import hashlib
from pathlib import Path
import requests
import pickle
import json
from collections import Counter, defaultdict
import subprocess
import sys
import calendar

# Java í™˜ê²½ë³€ìˆ˜ ì„¤ì • (konlpy ì‚¬ìš© ì „)
if not os.environ.get("JAVA_HOME"):
    os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-11-openjdk-amd64"

# konlpy importëŠ” í™˜ê²½ë³€ìˆ˜ ì„¤ì • í›„ì—
from konlpy.tag import Okt
okt = Okt()

# ìë™ íŒ¨í‚¤ì§€ ì„¤ì¹˜ í•¨ìˆ˜
def install_package(package):
    """íŒ¨í‚¤ì§€ ìë™ ì„¤ì¹˜"""
    try:
        subprocess.check_call([sys.executable, "-m", "pip", "install", package])
        return True
    except subprocess.CalledProcessError:
        return False

# ë¨¸ì‹ ëŸ¬ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬ import
def safe_import():
    """ê°œì„ ëœ ì•ˆì „í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ import"""
    global TfidfVectorizer, cosine_similarity
    
    # í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬
    try:
        from sklearn.feature_extraction.text import TfidfVectorizer
        from sklearn.metrics.pairwise import cosine_similarity
        st.success("âœ… ë¨¸ì‹ ëŸ¬ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì„±ê³µ")
    except ImportError:
        st.warning("âš ï¸ scikit-learnì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        TfidfVectorizer = None
        cosine_similarity = None

# ë¼ì´ë¸ŒëŸ¬ë¦¬ ì´ˆê¸°í™”
safe_import()

# êµ¬ê¸€ ì‹œíŠ¸ ìœ í‹¸ë¦¬í‹° ëª¨ë“ˆ
try:
    from google_sheets_utils_deploy import (
        load_data_from_google_sheets_cached,
        preprocess_dataframe,
        test_connection,
        get_google_sheets_client,
        get_connection_status
    )
    SHEETS_UTILS_AVAILABLE = True
except ImportError:
    SHEETS_UTILS_AVAILABLE = False

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="ğŸš€ ë‰´ìŠ¤íŒ©í† ë¦¬",
    page_icon="ğŸš€",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ì „ì—­ ì„¤ì •
PERPLEXITY_API_KEY = st.secrets["PERPLEXITY_API_KEY"]
APP_PASSWORD = st.secrets["APP_PASSWORD"]

# ìºì‹œ ì„¤ì •
CACHE_DIR = Path("cache")
CACHE_DIR.mkdir(exist_ok=True)
DATA_CACHE_DIR = CACHE_DIR / "data"
DATA_CACHE_DIR.mkdir(exist_ok=True)
ANALYSIS_CACHE_DIR = CACHE_DIR / "analysis"
ANALYSIS_CACHE_DIR.mkdir(exist_ok=True)
API_CACHE_DIR = CACHE_DIR / "api_calls"
API_CACHE_DIR.mkdir(exist_ok=True)

# ì–´ë‘ìš´ í…Œë§ˆ CSS ìŠ¤íƒ€ì¼
st.markdown("""
<style>
    .main-header {
        background: linear-gradient(90deg, #1e3c72, #2a5298);
        color: white;
        padding: 1rem;
        border-radius: 10px;
        margin-bottom: 2rem;
    }
    .metric-card {
        background: #f8f9fa;
        padding: 1rem;
        border-radius: 8px;
        border-left: 4px solid #007bff;
    }
    .success-box {
        background: #d4edda;
        border: 1px solid #c3e6cb;
        border-radius: 5px;
        padding: 1rem;
        margin: 1rem 0;
    }
    .warning-box {
        background: #fff3cd;
        border: 1px solid #ffeaa7;
        border-radius: 5px;
        padding: 1rem;
        margin: 1rem 0;
    }
    .error-box {
        background: #f8d7da;
        border: 1px solid #f5c6cb;
        border-radius: 5px;
        padding: 1rem;
        margin: 1rem 0;
    }
</style>
""", unsafe_allow_html=True)

# ================================
# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
# ================================
def initialize_session_state():
    """ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”"""
    if 'data_loaded' not in st.session_state:
        st.session_state.data_loaded = False
    if 'main_df' not in st.session_state:
        st.session_state.main_df = None
    if 'last_update' not in st.session_state:
        st.session_state.last_update = None
    if 'login_status' not in st.session_state:
        st.session_state.login_status = False

# ================================
# ìºì‹± ì‹œìŠ¤í…œ
# ================================
def get_cache_key(data_type, industry, date_str=None):
    """ìºì‹œ í‚¤ ìƒì„±"""
    if date_str is None:
        date_str = datetime.now().strftime("%Y-%m-%d")
    key_string = f"{data_type}_{industry}_{date_str}"
    return hashlib.md5(key_string.encode()).hexdigest()

def save_to_cache(cache_type, key, data):
    """ìºì‹œì— ë°ì´í„° ì €ì¥"""
    try:
        if cache_type == "data":
            cache_file = DATA_CACHE_DIR / f"{key}.pkl"
        elif cache_type == "analysis":
            cache_file = ANALYSIS_CACHE_DIR / f"{key}.pkl"
        else:
            cache_file = API_CACHE_DIR / f"{key}.pkl"
        
        cache_data = {
            'data': data,
            'timestamp': time.time(),
            'date': datetime.now().strftime("%Y-%m-%d")
        }
        
        with open(cache_file, 'wb') as f:
            pickle.dump(cache_data, f)
        return True
    except Exception as e:
        st.error(f"ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {str(e)}")
        return False

def load_from_cache(cache_type, key, max_age_hours=24):
    """ìºì‹œì—ì„œ ë°ì´í„° ë¡œë“œ"""
    try:
        if cache_type == "data":
            cache_file = DATA_CACHE_DIR / f"{key}.pkl"
        elif cache_type == "analysis":
            cache_file = ANALYSIS_CACHE_DIR / f"{key}.pkl"
        else:
            cache_file = API_CACHE_DIR / f"{key}.pkl"
        
        if not cache_file.exists():
            return None
        
        with open(cache_file, 'rb') as f:
            cache_data = pickle.load(f)
        
        # ìºì‹œ ìœ íš¨ì„± ê²€ì‚¬
        cache_age = time.time() - cache_data['timestamp']
        if cache_age > max_age_hours * 3600:
            return None
        
        return cache_data['data']
    except:
        return None

# ================================
# ë°ì´í„° ë¡œë”© í•¨ìˆ˜ (ì„¸ì…˜ ìƒíƒœ ê´€ë¦¬)
# ================================
def load_data_with_session_management(force_refresh=False):
    """ì„¸ì…˜ ìƒíƒœë¥¼ í™œìš©í•œ ë°ì´í„° ë¡œë”©"""
    # ì´ë¯¸ ë¡œë“œëœ ë°ì´í„°ê°€ ìˆê³  ê°•ì œ ìƒˆë¡œê³ ì¹¨ì´ ì•„ë‹Œ ê²½ìš°
    if not force_refresh and st.session_state.data_loaded and st.session_state.main_df is not None:
        st.info("ğŸ’¾ ì €ì¥ëœ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        return st.session_state.main_df
    
    # ìƒˆë¡œìš´ ë°ì´í„° ë¡œë“œ
    with st.spinner("ğŸ“Š êµ¬ê¸€ ì‹œíŠ¸ì—ì„œ ë°ì´í„° ë¡œë”© ì¤‘..."):
        if SHEETS_UTILS_AVAILABLE:
            df = load_data_from_google_sheets_cached(force_refresh=force_refresh)
            if not df.empty:
                df = preprocess_dataframe(df)
                # ì„¸ì…˜ ìƒíƒœì— ì €ì¥
                st.session_state.main_df = df
                st.session_state.data_loaded = True
                st.session_state.last_update = datetime.now()
                st.success(f"âœ… ë°ì´í„° ë¡œë”© ì™„ë£Œ: {len(df)}ê°œ ê¸°ì‚¬")
                return df
            else:
                st.error("âŒ êµ¬ê¸€ ì‹œíŠ¸ì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return pd.DataFrame()
        else:
            st.error("âŒ êµ¬ê¸€ ì‹œíŠ¸ ì—°ê²° ëª¨ë“ˆì´ ì—†ìŠµë‹ˆë‹¤.")
            return pd.DataFrame()

# ================================
# API í˜¸ì¶œ
# ================================
def call_perplexity_api_cached(prompt, model="sonar-pro", max_age_hours=24):
    """ìºì‹±ëœ Perplexity API í˜¸ì¶œ"""
    try:
        # ìºì‹œ í‚¤ ìƒì„±
        cache_key = hashlib.md5(f"{prompt}_{model}".encode()).hexdigest()
        
        # ìºì‹œì—ì„œ ë¨¼ì € í™•ì¸
        cached_result = load_from_cache("api", cache_key, max_age_hours)
        if cached_result:
            st.info("ğŸ”„ ìºì‹œì—ì„œ AI ë¶„ì„ ê²°ê³¼ë¥¼ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.")
            return cached_result
        
        # API í˜¸ì¶œ
        headers = {
            "Authorization": f"Bearer {PERPLEXITY_API_KEY}",
            "Content-Type": "application/json"
        }
        
        data = {
            "model": model,
            "messages": [
                {
                    "role": "system",
                    "content": "ë‹¹ì‹ ì€ ë‰´ìŠ¤ ì „ë¬¸ ë¶„ì„ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ê¸°ì‚¬ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ í•µì‹¬ íŠ¸ë Œë“œì™€ ì¤‘ìš”í•œ í‚¤ì›Œë“œë¥¼ í•œêµ­ì–´ë¡œ ê°„ê²°í•˜ê²Œ ìš”ì•½í•´ì£¼ì„¸ìš”."
                },
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            "temperature": 0.3,
            "max_tokens": 1000
        }
        
        response = requests.post(
            "https://api.perplexity.ai/chat/completions",
            headers=headers,
            json=data,
            timeout=30
        )
        
        if response.status_code == 200:
            result = response.json()
            api_result = result["choices"][0]["message"]["content"]
            
            # ê²°ê³¼ë¥¼ ìºì‹œì— ì €ì¥
            save_to_cache("api", cache_key, api_result)
            st.success("ğŸ†• ìƒˆë¡œìš´ AI ë¶„ì„ì„ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤.")
            return api_result
        else:
            st.error(f"API ì˜¤ë¥˜: {response.status_code}")
            return None
            
    except Exception as e:
        st.error(f"API í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}")
        return None

# ================================
# ğŸ”§ NEW UTILS (ìƒˆë¡œìš´ ê¸°ëŠ¥ í•¨ìˆ˜ë“¤)
# ================================
def compile_articles_text(articles, max_each=3):
    """ìœ ì‚¬ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸ â†’ Promptìš© í…ìŠ¤íŠ¸ ë¬¸ìì—´"""
    texts = []
    for art in articles[:max_each]:
        title = art.get("ì œëª©", "")
        body = art.get("ì£¼ìš”ë‚´ìš©", "")
        if pd.notna(title):
            texts.append(f"ì œëª©: {title}\në‚´ìš©: {str(body)[:300]}")
    return "\n".join(texts)

def generate_monthly_ai_plans(insight_dict, df, industry="ì „ì²´"):
    """
    ì£¼ì°¨ë³„ í•µì‹¬ë¬¸ì¥ + ìœ ì‚¬ ê¸°ì‚¬ë“¤ì„ ë¬¶ì–´ Perplexity APIë¡œ 'AI ì¶”ì²œ ê¸°íš ì•„ì´í…œ' ìƒì„±
    """
    if not insight_dict or df.empty:
        return "ë°ì´í„°ê°€ ë¶€ì¡±í•´ ê¸°íš ì•„ì´í…œì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    plan_source_blocks = []
    for week, sentence in insight_dict.get("weekly_insights", {}).items():
        if not sentence:
            continue
        sims = find_similar_articles_to_insight(df, sentence, 7)
        sim_text = compile_articles_text([s["article"] for s in sims])
        plan_source_blocks.append(
            f"[{week}] í•µì‹¬ë¬¸ì¥: {sentence}\nê´€ë ¨ ê¸°ì‚¬:\n{sim_text}"
        )
    
    prompt = f"""
    ë‹¹ì‹ ì€ í•œêµ­ ì£¼ìš” ì¼ê°„ì§€ì˜ {industry if industry!='ì „ì²´' else ''} ë‹´ë‹¹ ë¶€ì¥ì…ë‹ˆë‹¤.
    ì•„ë˜ ì£¼ì°¨ë³„ í•µì‹¬ íŠ¸ë Œë“œì™€ ê´€ë ¨ ê¸°ì‚¬ë“¤ì„ ë¶„ì„í•´ ë‹¤ìŒ ë‹¬ ì·¨ì¬ ë°©í–¥ì„ ì œì‹œí•  'AI ì¶”ì²œ ê¸°íš ì•„ì´í…œ' 5ê±´ì„ ì‘ì„±í•˜ì„¸ìš”.
    
    [ìš”ì²­ í˜•ì‹]
    ì œì‹œ ë¬¸ì¥(ê¸°ì‚¬ ì œëª© í›„ë³´) - í•œ ì¤„ ì´ìœ 
    
    [ë¶„ì„ ëŒ€ìƒ ë°ì´í„°]
    {os.linesep.join(plan_source_blocks[:3])}
    """
    
    return call_perplexity_api_cached(prompt, max_age_hours=6)

def generate_custom_topic_brief(df, industry, topic, weight_threshold=0.45):
    """
    ì—…ê³„+ì£¼ì œ ê´€ë ¨ ê¸°ì‚¬ ì¶”ì¶œ í›„ Perplexity APIë¡œ ê³ ê¸‰ ë¸Œë¦¬í•‘ ìƒì„±
    """
    if df.empty or not topic:
        return None
    
    # ì—…ê³„ í•„í„°
    pool = df if industry == "ì „ì²´" else df[df["ì—…ê³„"] == industry]
    if pool.empty:
        return None
    
    # ìœ ì‚¬ë„ ê³„ì‚°
    pool = pool.copy()
    pool["combined_text"] = pool["ì œëª©"].fillna("") + " " + pool["ì£¼ìš”ë‚´ìš©"].fillna("")
    
    if not TfidfVectorizer or not cosine_similarity:
        return "TF-IDF ë¶„ì„ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤."
    
    vec = TfidfVectorizer(max_features=1500)
    try:
        mat = vec.fit_transform(pool["combined_text"].tolist() + [topic])
        sims = cosine_similarity(mat[-1:], mat[:-1]).flatten()
        
        pool["sim"] = sims
        cand = pool[(pool["sim"] > 0.1) & (pool["ì „ì²´ê°€ì¤‘ì¹˜"] >= weight_threshold)]
        
        if cand.empty:
            return None
        
        cand = cand.sort_values(["sim", "ì „ì²´ê°€ì¤‘ì¹˜"], ascending=False).head(20)
        
        joined = [
            f"ì œëª©: {r['ì œëª©']}\në‚´ìš©: {str(r['ì£¼ìš”ë‚´ìš©'])[:300]}"
            for _, r in cand.iterrows()
        ]
        
        prompt = f"""
        ë‹¹ì‹ ì€ í•œêµ­ ì–¸ë¡ ì‚¬ì˜ {industry if industry!='ì „ì²´' else ''} ë‹´ë‹¹ í¸ì§‘êµ­ì¥ì…ë‹ˆë‹¤.
        ì£¼ì œ: {topic}
        
        ì•„ë˜ ê¸°ì‚¬ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ
        1) í˜„ì¬ í•µì‹¬ íŠ¸ë Œë“œ 3ê°€ì§€
        2) ê° íŠ¸ë Œë“œë³„ ë§¤ì²´ ë³´ë„ íŠ¹ì§•
        3) ì•ìœ¼ë¡œ ë°œêµ´í•´ì•¼ í•  ì‹¬ì¸µ ê¸°íš ê¸°ì‚¬ 3ê±´(ì œëª©+ê°„ë‹¨ ì´ìœ )
        ì„ ê¸°ìë“¤ì—ê²Œ ì§€ì‹œí•˜ëŠ” êµ¬ì–´ì²´ ë©”ëª¨ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.
        
        ê¸°ì‚¬ ëª©ë¡:
        {os.linesep.join(joined[:10])}
        """
        
        return call_perplexity_api_cached(prompt, max_age_hours=6)
        
    except:
        return "ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

# ================================
# ìƒˆë¡œìš´ AI ê¸°ëŠ¥ í•¨ìˆ˜ë“¤
# ================================
def generate_today_planning_items(df, industry="ì „ì²´"):
    """
    ìƒìœ„ ê°€ì¤‘ì¹˜ ê¸°ì‚¬ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ Perplexity APIë¥¼ í˜¸ì¶œ, 'ì˜¤ëŠ˜ì˜ ê¸°íš ì•„ì´í…œ' 3ê°œë¥¼ ì œì•ˆí•œë‹¤.
    """
    if df.empty:
        return "ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
    
    if industry != "ì „ì²´":
        df = df[df["ì—…ê³„"] == industry]
    
    # ê°€ì¤‘ì¹˜ ìƒìœ„ ê¸°ì‚¬ 20ê°œê¹Œì§€ ëŒ€ìƒ
    top_articles = (
        df.nlargest(20, "ì „ì²´ê°€ì¤‘ì¹˜") if "ì „ì²´ê°€ì¤‘ì¹˜" in df.columns else df.head(20)
    )
    
    texts = []
    for _, art in top_articles.iterrows():
        title = art.get("ì œëª©", "")
        body = art.get("ì£¼ìš”ë‚´ìš©", "")
        if pd.notna(title) and pd.notna(body):
            texts.append(f"ì œëª©: {title}\në‚´ìš©: {body}")
        elif pd.notna(title):
            texts.append(f"ì œëª©: {title}")
    
    if not texts:
        return "ë¶„ì„í•  ê¸°ì‚¬ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤."
    
    prompt = f"""
    ë‹¤ìŒì€ ì˜¤ëŠ˜ì˜ ì£¼ìš” ë‰´ìŠ¤ ê¸°ì‚¬ë“¤ì…ë‹ˆë‹¤. ê¸°ìë“¤ì´ ì¶”ê°€ ì·¨ì¬í•˜ê¸° ì¢‹ì€ 'ê¸°íš ì•„ì´í…œ' 3ê±´ì„ ì œì•ˆí•´ì£¼ì„¸ìš”.
    
    ê¸°ì‚¬ ëª©ë¡:
    {os.linesep.join(texts[:15])}
    
    [ë‹µë³€ í˜•ì‹]
    1. ê¸°íš ì•„ì´í…œ ì œëª©:
    - ì·¨ì¬ í¬ì¸íŠ¸:
    - ì˜ˆìƒ ì†ŒìŠ¤:
    
    2. ê¸°íš ì•„ì´í…œ ì œëª©:
    - ì·¨ì¬ í¬ì¸íŠ¸:
    - ì˜ˆìƒ ì†ŒìŠ¤:
    
    3. ê¸°íš ì•„ì´í…œ ì œëª©:
    - ì·¨ì¬ í¬ì¸íŠ¸:
    - ì˜ˆìƒ ì†ŒìŠ¤:
    """
    
    return (
        call_perplexity_api_cached(prompt, max_age_hours=6)
        or "ê¸°íš ì•„ì´í…œ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
    )

def generate_monthly_weekly_insights(df, target_month=None):
    """
    ì›”ë³„Â·ì£¼ì°¨ë³„ í•µì‹¬ ë¬¸ì¥ ìƒì„±(PPLX API) â†’ dict ë°˜í™˜
    {
        'monthly_insight': str,
        'weekly_insights': {ì£¼ì°¨: str, â€¦}
    }
    """
    if df.empty:
        return {}
    
    if target_month and "ì›”" in df.columns:
        df = df[df["ì›”"] == target_month]
    
    if df.empty:
        return {}
    
    # ì›” ì „ì²´ í•µì‹¬ ë¬¸ì¥
    month_txt = " ".join(
        (
            f"{r.get('ì œëª©','')} {r.get('ì£¼ìš”ë‚´ìš©','')}"
            for _, r in df.head(50).iterrows()
        )
    )
    
    mon_prompt = f"""
    ë‹¤ìŒì€ {target_month or 'ì´ë²ˆ ë‹¬'} ì£¼ìš” ë‰´ìŠ¤ì…ë‹ˆë‹¤.
    í•µì‹¬ ì´ìŠˆë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ ì£¼ì„¸ìš”.
    
    ë‰´ìŠ¤ ë‚´ìš©:
    {month_txt}
    """
    
    monthly_insight = call_perplexity_api_cached(mon_prompt, max_age_hours=12)
    
    # ì£¼ì°¨ë³„
    weekly_insights = {}
    if "ì£¼ì°¨" in df.columns:
        for week in sorted(df["ì£¼ì°¨"].dropna().unique()):
            w_df = df[df["ì£¼ì°¨"] == week]
            week_txt = " ".join(
                (
                    f"{r.get('ì œëª©','')} {r.get('ì£¼ìš”ë‚´ìš©','')}"
                    for _, r in w_df.head(20).iterrows()
                )
            )
            
            w_prompt = f"""
            ë‹¤ìŒì€ {week} ì£¼ìš” ë‰´ìŠ¤ì…ë‹ˆë‹¤.
            í•µì‹¬ ì´ìŠˆë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ ì£¼ì„¸ìš”.
            
            ë‰´ìŠ¤ ë‚´ìš©:
            {week_txt}
            """
            
            weekly_insights[week] = call_perplexity_api_cached(
                w_prompt, max_age_hours=24
            )
    
    return {"monthly_insight": monthly_insight, "weekly_insights": weekly_insights}

def find_similar_articles_to_insight(df, insight, top_n=5):
    """
    í•µì‹¬ ë¬¸ì¥(insight) ê³¼ ìœ ì‚¬ë„ê°€ ë†’ì€ ê¸°ì‚¬ nê°œ ë°˜í™˜
    """
    if df.empty or not insight:
        return []
    
    df = df.copy()
    df["combined_text"] = df.apply(
        lambda r: f"{r.get('ì œëª©','')} {r.get('ì£¼ìš”ë‚´ìš©','')}", axis=1
    )
    
    df = df[df["combined_text"].str.strip() != ""]
    
    if df.empty or not (TfidfVectorizer and cosine_similarity):
        return []
    
    try:
        vec = TfidfVectorizer(max_features=1000)
        mat = vec.fit_transform(df["combined_text"].tolist() + [insight])
        sims = cosine_similarity(mat[-1:], mat[:-1]).flatten()
        
        idxs = sims.argsort()[::-1][:top_n]
        return [
            {"article": df.iloc[i], "similarity": sims[i]}
            for i in idxs
            if sims[i] > 0.1
        ]
    except:
        return []

# ================================
# konlpy ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ (soynlp ëŒ€ì²´)
# ================================
def extract_keywords_okt(text_list, top_n=15):
    """konlpy Oktë¥¼ ì‚¬ìš©í•œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
    try:
        if not text_list:
            return []
        
        # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
        if isinstance(text_list, str):
            text_list = [text_list]
        
        # í…ìŠ¤íŠ¸ ì •ì œ
        cleaned_texts = []
        for text in text_list:
            if pd.notna(text):
                # ê¸°ë³¸ ì •ì œ
                text = str(text).strip()
                text = re.sub(r'[^\w\sê°€-í£]', ' ', text)
                text = re.sub(r'\s+', ' ', text)
                if len(text) > 10:  # ë„ˆë¬´ ì§§ì€ í…ìŠ¤íŠ¸ ì œì™¸
                    cleaned_texts.append(text)
        
        if not cleaned_texts:
            return []
        
        # ì „ì²´ í…ìŠ¤íŠ¸ ê²°í•©
        corpus = ' '.join(cleaned_texts)
        
        # ëª…ì‚¬ ì¶”ì¶œ
        nouns = okt.nouns(corpus)
        
        # ëª…ì‚¬ ë¹ˆë„ ê³„ì‚°
        noun_freq = Counter(nouns)
        
        # ë¶ˆìš©ì–´ ì œê±°
        stopwords = {
            'ê¸°ì', 'ì‚¬ì§„', 'ì œê³µ', 'ê´€ë ¨', 'ì—…ê³„', 'ì‹œì¥', 'ë¶„ì•¼', 'íšŒì‚¬', 'ê¸°ì—…',
            'ë°œí‘œ', 'ê³µê°œ', 'ì„¤ëª…', 'ë³´ë„', 'ë‰´ìŠ¤', 'ê¸°ì‚¬', 'ì˜¤ëŠ˜', 'ì–´ì œ', 'ë‚´ì¼',
            'ìµœê·¼', 'í˜„ì¬', 'ë‹¹ì‹œ', 'ì´ë²ˆ', 'ë‹¤ìŒ', 'ì§€ë‚œ', 'êµ­ë‚´', 'í•´ì™¸', 'ì „êµ­',
            'ê²ƒ', 'ìˆ˜', 'ë“±', 'ë°', 'ë˜', 'ë”', 'ì´', 'ê·¸', 'ì„', 'ë¥¼', 'ì˜', 'ê°€',
            'ë§í–ˆë‹¤', 'ë°í˜”ë‹¤', 'ì „í–ˆë‹¤', 'í–ˆë‹¤', 'ìˆë‹¤', 'ì—†ë‹¤', 'í•œë‹¤', 'ëœë‹¤'
        }
        
        # ìµœì¢… í‚¤ì›Œë“œ ì„ ë³„
        final_keywords = []
        for word, freq in noun_freq.most_common():
            if (word not in stopwords and 
                not word.isdigit() and 
                len(word) >= 2 and 
                len(word) <= 10 and 
                re.search(r'[ê°€-í£]', word)):
                final_keywords.append((word, freq))
        
        return final_keywords[:top_n]
    
    except Exception as e:
        st.error(f"í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
        return extract_keywords_simple(text_list, top_n)

def extract_keywords_simple(text_list, top_n=10):
    """ë°±ì—…ìš© ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
    try:
        if isinstance(text_list, str):
            text_list = [text_list]
        
        all_text = ' '.join([str(text) for text in text_list if pd.notna(text)])
        
        # ê°„ë‹¨í•œ í† í°í™”
        words = re.findall(r'[ê°€-í£]{2,6}', all_text)
        
        # ë¶ˆìš©ì–´ ì œê±°
        stopwords = {
            'ê¸°ì', 'ì‚¬ì§„', 'ì œê³µ', 'ê´€ë ¨', 'ì—…ê³„', 'ì‹œì¥', 'ë¶„ì•¼', 'íšŒì‚¬', 'ê¸°ì—…',
            'ë°œí‘œ', 'ê³µê°œ', 'ì„¤ëª…', 'ë³´ë„', 'ë‰´ìŠ¤', 'ê¸°ì‚¬', 'ì˜¤ëŠ˜', 'ì–´ì œ', 'ë‚´ì¼'
        }
        
        keywords = [word for word in words if word not in stopwords]
        word_freq = Counter(keywords)
        
        return word_freq.most_common(top_n)
    except:
        return []

# ê¸°ì¡´ extract_keywords_mecab í•¨ìˆ˜ ëŒ€ì²´
def extract_keywords_mecab(text, top_n=10):
    """í‚¤ì›Œë“œ ì¶”ì¶œ - konlpy Okt ì‚¬ìš©"""
    if isinstance(text, str):
        return extract_keywords_okt([text], top_n)
    elif isinstance(text, list):
        return extract_keywords_okt(text, top_n)
    else:
        return extract_keywords_okt([str(text)], top_n)

# ================================
# ì—…ê³„ë³„ í‚¤ì›Œë“œ ì¶”ì¶œ ê¸°ëŠ¥
# ================================
def extract_industry_keywords(df, industry):
    """ì—…ê³„ë³„ íŠ¹í™” í‚¤ì›Œë“œ ì¶”ì¶œ"""
    if industry != 'ì „ì²´':
        industry_articles = df[df['ì—…ê³„'] == industry]
    else:
        industry_articles = df
    
    titles = industry_articles['ì œëª©'].dropna().tolist()
    
    # konlpy Oktë¡œ ì—…ê³„ íŠ¹í™” í‚¤ì›Œë“œ ì¶”ì¶œ
    keywords = extract_keywords_okt(titles, 20)
    
    # ì—…ê³„ë³„ ê°€ì¤‘ì¹˜ ì¡°ì •
    industry_weights = {
        'ìë™ì°¨': ['ì „ê¸°ì°¨', 'ììœ¨ì£¼í–‰', 'ë°°í„°ë¦¬', 'ëª¨ë¹Œë¦¬í‹°', 'ì „ë™í™”'],
        'IT/ì „ì': ['AI', 'ë°˜ë„ì²´', 'í´ë¼ìš°ë“œ', 'ë””ì§€í„¸', 'ì¸ê³µì§€ëŠ¥'],
        'ê¸ˆìœµ': ['í•€í…Œí¬', 'ê°€ìƒí™”í', 'ë¸”ë¡ì²´ì¸', 'íˆ¬ì', 'ë””ì§€í„¸ë±…í‚¹'],
        'ë°”ì´ì˜¤': ['ë°±ì‹ ', 'ì¹˜ë£Œì œ', 'ì„ìƒ', 'ì‹ ì•½', 'ì˜ë£Œ'],
        'í™”í•™/ì—ë„ˆì§€': ['ì¹œí™˜ê²½', 'íƒ„ì†Œì¤‘ë¦½', 'ì¬ìƒì—ë„ˆì§€', 'ìˆ˜ì†Œ'],
        'í•­ê³µ/ìš´ì†¡': ['ë¬¼ë¥˜', 'ë°°ì†¡', 'ëª¨ë¹Œë¦¬í‹°', 'ìš´ì†¡'],
        'ê±´ì„¤/ë¶€ë™ì‚°': ['ê°œë°œ', 'ë¶„ì–‘', 'ì¬ê±´ì¶•', 'ë¦¬ëª¨ë¸ë§'],
        'ìœ í†µ/ì†Œë¹„ì¬': ['ì´ì»¤ë¨¸ìŠ¤', 'ì˜¨ë¼ì¸', 'ë°°ì†¡', 'ì†Œë¹„'],
        'ì—”í„°í…Œì¸ë¨¼íŠ¸/ë¯¸ë””ì–´': ['ì½˜í…ì¸ ', 'í”Œë«í¼', 'ìŠ¤íŠ¸ë¦¬ë°', 'OTT'],
        'ê²Œì„': ['ë©”íƒ€ë²„ìŠ¤', 'ê²Œì„', 'í”Œë«í¼', 'ëª¨ë°”ì¼'],
        'ì‹í’ˆ/ìŒë£Œ': ['ê±´ê°•', 'í”„ë¦¬ë¯¸ì—„', 'ì¹œí™˜ê²½', 'ìœ ê¸°ë†']
    }
    
    if industry in industry_weights:
        boost_keywords = industry_weights[industry]
        enhanced_keywords = []
        for word, score in keywords:
            if any(boost in word for boost in boost_keywords):
                enhanced_keywords.append((word, score * 1.5))
            else:
                enhanced_keywords.append((word, score))
        return sorted(enhanced_keywords, key=lambda x: x[1], reverse=True)
    
    return keywords

# ================================
# ë„¤ì´ë²„ ë§í¬ í•˜ì´í¼ë§í¬ ì²˜ë¦¬ í•¨ìˆ˜ (í•µì‹¬ ìˆ˜ì • ë¶€ë¶„)
# ================================
def get_naver_link_column(df):
    """êµ¬ê¸€ ì‹œíŠ¸ì—ì„œ ë„¤ì´ë²„ ë§í¬ ì»¬ëŸ¼ëª…ì„ ì°¾ëŠ” í•¨ìˆ˜"""
    possible_columns = ['ë„¤ì´ë²„ë§í¬', 'ë„¤ì´ë²„ ë§í¬', 'ë„¤ì´ë²„URL', 'ë„¤ì´ë²„ URL', 'naver_link', 'naver_url']
    
    for col in possible_columns:
        if col in df.columns:
            return col
    
    # ìœ ì‚¬í•œ ì»¬ëŸ¼ëª… ì°¾ê¸°
    for col in df.columns:
        if 'ë„¤ì´ë²„' in str(col) and ('ë§í¬' in str(col) or 'URL' in str(col)):
            return col
    
    return None

def get_media_link_column(df):
    """êµ¬ê¸€ ì‹œíŠ¸ì—ì„œ ë§¤ì²´ë§í¬ ì»¬ëŸ¼ëª…ì„ ì°¾ëŠ” í•¨ìˆ˜"""
    possible_columns = ['ë§¤ì²´ë§í¬', 'ë§¤ì²´ ë§í¬', 'ë§¤ì²´URL', 'ë§¤ì²´ URL', 'media_link', 'media_url']
    
    for col in possible_columns:
        if col in df.columns:
            return col
    
    # ìœ ì‚¬í•œ ì»¬ëŸ¼ëª… ì°¾ê¸°
    for col in df.columns:
        if 'ë§¤ì²´' in str(col) and ('ë§í¬' in str(col) or 'URL' in str(col)):
            return col
    
    return None

def create_article_hyperlink(title, naver_url=None, media_url=None):
    """ê¸°ì‚¬ ì œëª©ì— í•˜ì´í¼ë§í¬ë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜"""
    if pd.isna(title):
        title = "ì œëª© ì—†ìŒ"
    else:
        title = str(title).strip()
    
    # 1ìˆœìœ„: ë„¤ì´ë²„ë§í¬ ì‚¬ìš©
    if pd.notna(naver_url) and str(naver_url).strip() and str(naver_url).strip().lower() not in ['nan', '', 'none']:
        naver_url = str(naver_url).strip()
        if not naver_url.startswith(('http://', 'https://')):
            naver_url = 'https://' + naver_url
        return f'ğŸ“° [{title}]({naver_url})'
    
    # 2ìˆœìœ„: ë§¤ì²´ë§í¬ ì‚¬ìš©
    if pd.notna(media_url) and str(media_url).strip() and str(media_url).strip().lower() not in ['nan', '', 'none']:
        media_url = str(media_url).strip()
        if not media_url.startswith(('http://', 'https://')):
            media_url = 'https://' + media_url
        return f'ğŸ“° [{title}]({media_url})'
    
    return f"ğŸ“„ {title}"

def display_article_with_link(article, df=None):
    """ê¸°ì‚¬ë¥¼ í•˜ì´í¼ë§í¬ì™€ í•¨ê»˜ í‘œì‹œí•˜ëŠ” í•¨ìˆ˜"""
    title = article.get('ì œëª©', 'ì œëª© ì—†ìŒ')
    
    # ë„¤ì´ë²„ ë§í¬ ì»¬ëŸ¼ëª… ìë™ ê°ì§€
    if df is not None:
        naver_col = get_naver_link_column(df)
        naver_url = article.get(naver_col) if naver_col else None
        
        # ë§¤ì²´ ë§í¬ ì»¬ëŸ¼ëª… ìë™ ê°ì§€
        media_col = get_media_link_column(df)
        media_url = article.get(media_col) if media_col else None
    else:
        naver_url = article.get('ë„¤ì´ë²„ë§í¬') or article.get('ë„¤ì´ë²„ URL')
        media_url = article.get('ë§¤ì²´ë§í¬') or article.get('ë§¤ì²´ URL')
    
    # í•˜ì´í¼ë§í¬ê°€ ì ìš©ëœ ì œëª© ìƒì„±
    linked_title = create_article_hyperlink(title, naver_url, media_url)
    
    return linked_title

# ================================
# PyArrow ì˜¤ë¥˜ í•´ê²°ìš© ì•ˆì „í•œ í…Œì´ë¸” í‘œì‹œ í•¨ìˆ˜
# ================================
def safe_display_dataframe(df, title="ë°ì´í„°"):
    """PyArrow ì˜¤ë¥˜ë¥¼ í”¼í•œ ì•ˆì „í•œ ë°ì´í„°í”„ë ˆì„ í‘œì‹œ"""
    try:
        # ë¨¼ì € st.dataframe ì‹œë„
        st.dataframe(df, use_container_width=True)
    except Exception as e:
        # PyArrow ì˜¤ë¥˜ ì‹œ ëŒ€ì•ˆ í‘œì‹œ
        st.warning(f"âš ï¸ í‘œ í‘œì‹œ ì˜¤ë¥˜ (PyArrow): {str(e)}")
        st.markdown(f"### ğŸ“Š {title}")
        
        # HTML í…Œì´ë¸”ë¡œ ëŒ€ì²´
        html_table = "<table style='width:100%'><tr>"
        for col in df.columns:
            html_table += f"<th>{col}</th>"
        html_table += "</tr>"
        
        for _, row in df.head(10).iterrows():
            html_table += "<tr>"
            for col in df.columns:
                html_table += f"<td>{row[col]}</td>"
            html_table += "</tr>"
        html_table += "</table>"
        
        st.markdown(html_table, unsafe_allow_html=True)

# ================================
# ë¡œê·¸ì¸ í•¨ìˆ˜
# ================================
def login():
    """ë¡œê·¸ì¸ ì²˜ë¦¬"""
    st.title("ğŸ” ë‰´ìŠ¤íŒ©í† ë¦¬ ë¡œê·¸ì¸")
    
    password = st.text_input("ë¹„ë°€ë²ˆí˜¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”", type="password")
    
    if st.button("ë¡œê·¸ì¸"):
        if password == APP_PASSWORD:
            st.session_state.login_status = True
            st.success("âœ… ë¡œê·¸ì¸ ì„±ê³µ!")
            st.rerun()
        else:
            st.error("âŒ ë¹„ë°€ë²ˆí˜¸ê°€ í‹€ë ¸ìŠµë‹ˆë‹¤.")

# ================================
# ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜
# ================================
def main():
    """ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜"""
    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    initialize_session_state()
    
    # ë¡œê·¸ì¸ í™•ì¸
    if not st.session_state.login_status:
        login()
        return
    
    # í—¤ë”
    st.markdown("""
    <div class="main-header">
        <h1>ğŸš€ ë‰´ìŠ¤íŒ©í† ë¦¬</h1>
        <p>AI ê¸°ë°˜ ê¸°ì‚¬ ë¶„ì„ í”Œë«í¼</p>
        <p>âœ¨ konlpy ê¸°ë°˜ í•œêµ­ì–´ ì²˜ë¦¬ ì‹œìŠ¤í…œ + ë°±ì—…íŒŒì¼ ê¸°ëŠ¥ ë³µì›</p>
    </div>
    """, unsafe_allow_html=True)
    
    # ì‚¬ì´ë“œë°”
    with st.sidebar:
        st.header("ğŸ”§ ì œì–´íŒ")
        
        # ë°ì´í„° ìƒˆë¡œê³ ì¹¨ ë²„íŠ¼
        if st.button("ğŸ”„ ë°ì´í„° ìƒˆë¡œê³ ì¹¨"):
            st.session_state.data_loaded = False
            st.session_state.main_df = None
            st.rerun()
        
        # ë°ì´í„° ë¡œë“œ ìƒíƒœ í‘œì‹œ
        if st.session_state.data_loaded and st.session_state.last_update:
            st.success(f"ğŸ“Š ë°ì´í„° ë¡œë“œë¨")
            st.info(f"ğŸ•’ ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸: {st.session_state.last_update.strftime('%Y-%m-%d %H:%M:%S')}")
        
        # êµ¬ê¸€ ì‹œíŠ¸ ì—°ê²° ìƒíƒœ
        if SHEETS_UTILS_AVAILABLE:
            connection_status = get_connection_status()
            if connection_status['connected']:
                st.success("âœ… êµ¬ê¸€ ì‹œíŠ¸ ì—°ê²°ë¨")
            else:
                st.error(f"âŒ êµ¬ê¸€ ì‹œíŠ¸ ì—°ê²° ì‹¤íŒ¨: {connection_status['message']}")
        
        # ìºì‹œ ì •ë¦¬
        if st.button("ğŸ—‘ï¸ ìºì‹œ ì •ë¦¬"):
            try:
                for cache_dir in [DATA_CACHE_DIR, ANALYSIS_CACHE_DIR, API_CACHE_DIR]:
                    for file in cache_dir.glob("*.pkl"):
                        file.unlink()
                st.success("âœ… ìºì‹œ ì •ë¦¬ ì™„ë£Œ")
            except Exception as e:
                st.error(f"âŒ ìºì‹œ ì •ë¦¬ ì‹¤íŒ¨: {str(e)}")
    
    # ë°ì´í„° ë¡œë“œ
    df = load_data_with_session_management()
    
    if df.empty:
        st.error("âŒ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ë©”ì¸ íƒ­
    tab1, tab2, tab3, tab4, tab5 = st.tabs([
        "ğŸ“Š ë°ì´í„° ë¶„ì„", "ğŸ” í‚¤ì›Œë“œ ë¶„ì„", "ğŸ¤– AI ì¸ì‚¬ì´íŠ¸", "ğŸ“ˆ íŠ¸ë Œë“œ ë¶„ì„", "ğŸ¯ ê¸°íš ì•„ì´í…œ"
    ])
    
    with tab1:
        st.subheader("ğŸ“Š ë°ì´í„° ë¶„ì„")
        
        # ì—…ê³„ ì„ íƒ
        industries = ['ì „ì²´'] + sorted(df['ì—…ê³„'].unique().tolist())
        selected_industry = st.selectbox("ì—…ê³„ ì„ íƒ", industries)
        
        # í•„í„°ë§ëœ ë°ì´í„°
        filtered_df = df if selected_industry == 'ì „ì²´' else df[df['ì—…ê³„'] == selected_industry]
        
        # ê¸°ë³¸ í†µê³„
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("ì „ì²´ ê¸°ì‚¬ ìˆ˜", len(filtered_df))
        with col2:
            st.metric("í‰ê·  ê°€ì¤‘ì¹˜", f"{filtered_df['ì „ì²´ê°€ì¤‘ì¹˜'].mean():.3f}")
        with col3:
            st.metric("ìµœê³  ê°€ì¤‘ì¹˜", f"{filtered_df['ì „ì²´ê°€ì¤‘ì¹˜'].max():.3f}")
        with col4:
            st.metric("ë§¤ì²´ ìˆ˜", filtered_df['ë§¤ì²´'].nunique())
        
        # ìƒìœ„ ê¸°ì‚¬ í‘œì‹œ
        st.subheader("â­ ìƒìœ„ ê°€ì¤‘ì¹˜ ê¸°ì‚¬")
        top_articles = filtered_df.nlargest(10, 'ì „ì²´ê°€ì¤‘ì¹˜')[['ì œëª©', 'ë§¤ì²´', 'ì „ì²´ê°€ì¤‘ì¹˜']]
        
        # ë§í¬ì™€ í•¨ê»˜ í‘œì‹œ
        for idx, article in top_articles.iterrows():
            article_data = filtered_df.loc[idx]
            linked_title = display_article_with_link(article_data, df)
            st.markdown(f"**{linked_title}**")
            st.write(f"ë§¤ì²´: {article['ë§¤ì²´']} | ê°€ì¤‘ì¹˜: {article['ì „ì²´ê°€ì¤‘ì¹˜']:.3f}")
            st.write("---")
        
        # ë§¤ì²´ë³„ ë¶„í¬
        st.subheader("ğŸ“° ë§¤ì²´ë³„ ë¶„í¬")
        media_dist = filtered_df['ë§¤ì²´'].value_counts().head(10)
        st.bar_chart(media_dist)
        
        # ì—…ê³„ë³„ ë¶„í¬
        if selected_industry == 'ì „ì²´':
            st.subheader("ğŸ¢ ì—…ê³„ë³„ ë¶„í¬")
            industry_dist = df['ì—…ê³„'].value_counts()
            st.bar_chart(industry_dist)
    
    with tab2:
        st.subheader("ğŸ” í‚¤ì›Œë“œ ë¶„ì„")
        
        # ì—…ê³„ ì„ íƒ
        industries = ['ì „ì²´'] + sorted(df['ì—…ê³„'].unique().tolist())
        selected_industry = st.selectbox("ì—…ê³„ ì„ íƒ", industries, key="keyword_industry")
        
        # í‚¤ì›Œë“œ ì¶”ì¶œ
        with st.spinner("ğŸ” í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘..."):
            keywords = extract_industry_keywords(df, selected_industry)
        
        if keywords:
            # í‚¤ì›Œë“œ ë°ì´í„°í”„ë ˆì„ ìƒì„±
            keyword_df = pd.DataFrame(keywords, columns=['í‚¤ì›Œë“œ', 'ë¹ˆë„'])
            
            st.subheader(f"ğŸ·ï¸ {selected_industry} ì£¼ìš” í‚¤ì›Œë“œ")
            
            # í‚¤ì›Œë“œ ì°¨íŠ¸
            col1, col2 = st.columns([2, 1])
            with col1:
                st.bar_chart(keyword_df.set_index('í‚¤ì›Œë“œ')['ë¹ˆë„'])
            with col2:
                st.dataframe(keyword_df)
        else:
            st.warning("âš ï¸ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    with tab3:
        st.subheader("ğŸ¤– AI ì¸ì‚¬ì´íŠ¸")
        
        # ì—…ê³„ ì„ íƒ
        industries = ['ì „ì²´'] + sorted(df['ì—…ê³„'].unique().tolist())
        selected_industry = st.selectbox("ì—…ê³„ ì„ íƒ", industries, key="ai_industry")
        
        # ì˜¤ëŠ˜ì˜ ê¸°íš ì•„ì´í…œ
        st.subheader("ğŸ“‹ ì˜¤ëŠ˜ì˜ ê¸°íš ì•„ì´í…œ")
        if st.button("ğŸ¤– AI ê¸°íš ì•„ì´í…œ ìƒì„±"):
            with st.spinner("ğŸ¤– AI ë¶„ì„ ì¤‘..."):
                planning_items = generate_today_planning_items(df, selected_industry)
            st.markdown(planning_items)
        
        # ì›”ë³„/ì£¼ì°¨ë³„ ì¸ì‚¬ì´íŠ¸
        st.subheader("ğŸ“… ì›”ë³„/ì£¼ì°¨ë³„ ì¸ì‚¬ì´íŠ¸")
        if st.button("ğŸ¤– ì¸ì‚¬ì´íŠ¸ ìƒì„±"):
            with st.spinner("ğŸ¤– AI ë¶„ì„ ì¤‘..."):
                insights = generate_monthly_weekly_insights(df)
            
            if insights.get('monthly_insight'):
                st.markdown("### ğŸ“Š ì›”ë³„ í•µì‹¬ ì¸ì‚¬ì´íŠ¸")
                st.markdown(insights['monthly_insight'])
            
            if insights.get('weekly_insights'):
                st.markdown("### ğŸ“Š ì£¼ì°¨ë³„ í•µì‹¬ ì¸ì‚¬ì´íŠ¸")
                for week, insight in insights['weekly_insights'].items():
                    st.markdown(f"**{week}**: {insight}")
        
        # ë§ì¶¤í˜• ì£¼ì œ ë¸Œë¦¬í•‘
        st.subheader("ğŸ¯ ë§ì¶¤í˜• ì£¼ì œ ë¸Œë¦¬í•‘")
        topic = st.text_input("ë¶„ì„í•  ì£¼ì œë¥¼ ì…ë ¥í•˜ì„¸ìš”", placeholder="ì˜ˆ: ì¸ê³µì§€ëŠ¥, ì „ê¸°ì°¨, ë°”ì´ì˜¤")
        
        if st.button("ğŸ¤– ì£¼ì œ ë¸Œë¦¬í•‘ ìƒì„±") and topic:
            with st.spinner("ğŸ¤– AI ë¶„ì„ ì¤‘..."):
                topic_brief = generate_custom_topic_brief(df, selected_industry, topic)
            if topic_brief:
                st.markdown(topic_brief)
    
    with tab4:
        st.subheader("ğŸ“ˆ íŠ¸ë Œë“œ ë¶„ì„")
        
        # ì›”ë³„ íŠ¸ë Œë“œ
        if 'ì›”' in df.columns:
            st.subheader("ğŸ“… ì›”ë³„ ê¸°ì‚¬ ìˆ˜")
            monthly_trend = df['ì›”'].value_counts().sort_index()
            st.line_chart(monthly_trend)
        
        # ì£¼ì°¨ë³„ íŠ¸ë Œë“œ
        if 'ì£¼ì°¨' in df.columns:
            st.subheader("ğŸ“… ì£¼ì°¨ë³„ ê¸°ì‚¬ ìˆ˜")
            weekly_trend = df['ì£¼ì°¨'].value_counts().sort_index()
            st.line_chart(weekly_trend)
        
        # ê°€ì¤‘ì¹˜ ë¶„í¬
        st.subheader("âš–ï¸ ê°€ì¤‘ì¹˜ ë¶„í¬")
        st.histogram_chart(df['ì „ì²´ê°€ì¤‘ì¹˜'])
    
    with tab5:
        st.subheader("ğŸ¯ ê¸°íš ì•„ì´í…œ")
        
        # ì—…ê³„ ì„ íƒ
        industries = ['ì „ì²´'] + sorted(df['ì—…ê³„'].unique().tolist())
        selected_industry = st.selectbox("ì—…ê³„ ì„ íƒ", industries, key="planning_industry")
        
        # ê¸°íš ì•„ì´í…œ ìƒì„±
        if st.button("ğŸ¯ ì›”ê°„ ê¸°íš ì•„ì´í…œ ìƒì„±"):
            with st.spinner("ğŸ¤– AI ê¸°íš ì•„ì´í…œ ìƒì„± ì¤‘..."):
                # ì›”ë³„/ì£¼ì°¨ë³„ ì¸ì‚¬ì´íŠ¸ ë¨¼ì € ìƒì„±
                insights = generate_monthly_weekly_insights(df)
                
                if insights:
                    # ê¸°íš ì•„ì´í…œ ìƒì„±
                    planning_items = generate_monthly_ai_plans(insights, df, selected_industry)
                    st.markdown("### ğŸ“‹ AI ì¶”ì²œ ê¸°íš ì•„ì´í…œ")
                    st.markdown(planning_items)
                else:
                    st.warning("âš ï¸ ì¸ì‚¬ì´íŠ¸ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        # ìœ ì‚¬ ê¸°ì‚¬ ê²€ìƒ‰
        st.subheader("ğŸ” ìœ ì‚¬ ê¸°ì‚¬ ê²€ìƒ‰")
        search_query = st.text_input("ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš”", placeholder="ì˜ˆ: ì¸ê³µì§€ëŠ¥ ê·œì œ")
        
        if st.button("ğŸ” ìœ ì‚¬ ê¸°ì‚¬ ê²€ìƒ‰") and search_query:
            similar_articles = find_similar_articles_to_insight(df, search_query, 10)
            
            if similar_articles:
                st.markdown("### ğŸ” ê²€ìƒ‰ ê²°ê³¼")
                for i, result in enumerate(similar_articles, 1):
                    article = result['article']
                    similarity = result['similarity']
                    
                    linked_title = display_article_with_link(article, df)
                    st.markdown(f"**{i}. {linked_title}**")
                    st.write(f"ìœ ì‚¬ë„: {similarity:.3f} | ë§¤ì²´: {article.get('ë§¤ì²´', 'N/A')}")
                    if pd.notna(article.get('ì£¼ìš”ë‚´ìš©')):
                        st.write(f"ë‚´ìš©: {str(article['ì£¼ìš”ë‚´ìš©'])[:200]}...")
                    st.write("---")
            else:
                st.warning("âš ï¸ ê´€ë ¨ ê¸°ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()
