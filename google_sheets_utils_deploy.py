"""
Google Sheets ìœ í‹¸ë¦¬í‹° ëª¨ë“ˆ - konlpy ê¸°ë°˜ + ë°ì´í„° ìºì‹± ì‹œìŠ¤í…œ
"""

import streamlit as st
import pandas as pd
import gspread
import json
import os
import re
import hashlib
import pickle
import time
from datetime import datetime, timedelta
from collections import Counter
from pathlib import Path

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ konlpy Okt ë¡œë“œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
from konlpy.tag import Okt
okt = Okt()

# ================================
# ìºì‹± ì‹œìŠ¤í…œ ì„¤ì •
# ================================

# ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
SHEETS_CACHE_DIR = Path("cache/sheets_data")
SHEETS_CACHE_DIR.mkdir(parents=True, exist_ok=True)

SHEETS_CACHE_FILE = SHEETS_CACHE_DIR / "google_sheets_cache.pkl"
SHEETS_METADATA_FILE = SHEETS_CACHE_DIR / "sheets_metadata.json"

# ================================
# ì„¤ì •
# ================================

# ì „ì—­ ì„¤ì •
try:
    SPREADSHEET_ID = st.secrets["SPREADSHEET_ID"]
    if "SERVICE_ACCOUNT_INFO" in st.secrets:
        SERVICE_ACCOUNT_INFO = dict(st.secrets["SERVICE_ACCOUNT_INFO"])
    else:
        SERVICE_ACCOUNT_INFO = None
    SERVICE_ACCOUNT_FILE = None
except:
    SPREADSHEET_ID = os.environ.get("SPREADSHEET_ID", "1bCDIOk_QEf5Q56r0xmSmgTlGRvQB9Px3G-8HrUu0EiM")
    SERVICE_ACCOUNT_FILE = os.environ.get("SERVICE_ACCOUNT_FILE", "diesel-practice-290305-76df92cfd2bd.json")
    SERVICE_ACCOUNT_INFO = None

SCOPES = [
    'https://www.googleapis.com/auth/spreadsheets',
    'https://www.googleapis.com/auth/drive'
]

# ================================
# ì„œë¹„ìŠ¤ ê³„ì • ì²˜ë¦¬
# ================================

_service_account_cache = None

def get_service_account_info():
    """ì„œë¹„ìŠ¤ ê³„ì • ì •ë³´ ê°€ì ¸ì˜¤ê¸°"""
    global _service_account_cache
    
    if _service_account_cache is not None:
        return _service_account_cache
    
    # 1ìˆœìœ„: Streamlit secrets
    if SERVICE_ACCOUNT_INFO:
        _service_account_cache = SERVICE_ACCOUNT_INFO
        return SERVICE_ACCOUNT_INFO
    
    # 2ìˆœìœ„: ë¡œì»¬ íŒŒì¼
    possible_files = [
        "diesel-practice-290305-76df92cfd2bd.json",
        "diesel-practice-290305-ac532875b6e6.json",
        "service_account.json"
    ]
    
    for filename in possible_files:
        if os.path.exists(filename):
            try:
                with open(filename, 'r', encoding='utf-8') as f:
                    service_account_info = json.load(f)
                _service_account_cache = service_account_info
                print(f"âœ… {filename}ì—ì„œ ì„œë¹„ìŠ¤ ê³„ì • ì •ë³´ ë¡œë“œ ì„±ê³µ")
                return service_account_info
            except Exception as e:
                print(f"âŒ {filename} ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    print("âŒ ëª¨ë“  ì„œë¹„ìŠ¤ ê³„ì • íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨")
    return None

def get_google_sheets_client():
    """êµ¬ê¸€ ì‹œíŠ¸ í´ë¼ì´ì–¸íŠ¸ ìƒì„±"""
    try:
        service_account_info = get_service_account_info()
        if service_account_info is None:
            return None
        
        gc = gspread.service_account_from_dict(service_account_info)
        return gc
    except Exception as e:
        print(f"êµ¬ê¸€ ì‹œíŠ¸ í´ë¼ì´ì–¸íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
        return None

def get_connection_status():
    """ì—°ê²° ìƒíƒœ í™•ì¸"""
    try:
        gc = get_google_sheets_client()
        if gc is None:
            return {'connected': False, 'message': 'ì„œë¹„ìŠ¤ ê³„ì • ë¡œë“œ ì‹¤íŒ¨'}
        
        sheet = gc.open_by_key(SPREADSHEET_ID)
        worksheet = sheet.get_worksheet(0)
        
        # ì—°ê²° í…ŒìŠ¤íŠ¸
        records = worksheet.get_all_records()
        return {'connected': True, 'message': 'ì •ìƒ ì—°ê²°'}
    except Exception as e:
        return {'connected': False, 'message': str(e)}

# ================================
# ğŸš€ ê°œì„ ëœ ìºì‹± ì‹œìŠ¤í…œ
# ================================

def get_sheets_last_modified():
    """êµ¬ê¸€ ì‹œíŠ¸ì˜ ë§ˆì§€ë§‰ ìˆ˜ì • ì‹œê°„ í™•ì¸"""
    try:
        gc = get_google_sheets_client()
        if gc is None:
            return None
        
        sheet = gc.open_by_key(SPREADSHEET_ID)
        # êµ¬ê¸€ ì‹œíŠ¸ APIë¡œ ë§ˆì§€ë§‰ ìˆ˜ì • ì‹œê°„ í™•ì¸
        # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨íˆ í˜„ì¬ ì‹œê°„ì„ ì‚¬ìš© (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” Drive API ì‚¬ìš©)
        return datetime.now().timestamp()
    except:
        return None

def save_sheets_cache(df, metadata):
    """êµ¬ê¸€ ì‹œíŠ¸ ë°ì´í„°ë¥¼ ìºì‹œì— ì €ì¥"""
    try:
        # ë°ì´í„° ìºì‹œ ì €ì¥
        with open(SHEETS_CACHE_FILE, 'wb') as f:
            pickle.dump(df, f)
        
        # ë©”íƒ€ë°ì´í„° ì €ì¥
        with open(SHEETS_METADATA_FILE, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… êµ¬ê¸€ ì‹œíŠ¸ ë°ì´í„° ìºì‹œ ì €ì¥ ì™„ë£Œ: {len(df)}í–‰")
        return True
    except Exception as e:
        print(f"âŒ ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")
        return False

def load_sheets_cache():
    """ìºì‹œì—ì„œ êµ¬ê¸€ ì‹œíŠ¸ ë°ì´í„° ë¡œë“œ"""
    try:
        # ìºì‹œ íŒŒì¼ ì¡´ì¬ í™•ì¸
        if not SHEETS_CACHE_FILE.exists() or not SHEETS_METADATA_FILE.exists():
            return None, None
        
        # ë©”íƒ€ë°ì´í„° ë¡œë“œ
        with open(SHEETS_METADATA_FILE, 'r', encoding='utf-8') as f:
            metadata = json.load(f)
        
        # ë°ì´í„° ë¡œë“œ
        with open(SHEETS_CACHE_FILE, 'rb') as f:
            df = pickle.load(f)
        
        print(f"âœ… ìºì‹œì—ì„œ ë°ì´í„° ë¡œë“œ ì„±ê³µ: {len(df)}í–‰")
        return df, metadata
    except Exception as e:
        print(f"âŒ ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None, None

def is_cache_valid(metadata, max_age_hours=1):
    """ìºì‹œ ìœ íš¨ì„± ê²€ì‚¬"""
    if not metadata:
        return False
    
    # ì‹œê°„ ê¸°ë°˜ ìºì‹œ ìœ íš¨ì„±
    cache_time = metadata.get('timestamp', 0)
    current_time = time.time()
    
    if current_time - cache_time > max_age_hours * 3600:
        print(f"â° ìºì‹œê°€ {max_age_hours}ì‹œê°„ì„ ì´ˆê³¼í•˜ì—¬ ë§Œë£Œë¨")
        return False
    
    return True

def load_data_from_google_sheets():
    """êµ¬ê¸€ ì‹œíŠ¸ì—ì„œ ì§ì ‘ ë°ì´í„° ë¡œë“œ (ìºì‹œ ì—†ì´)"""
    try:
        gc = get_google_sheets_client()
        if gc is None:
            return pd.DataFrame()
        
        sheet = gc.open_by_key(SPREADSHEET_ID)
        worksheet = sheet.get_worksheet(0)
        
        data = worksheet.get_all_records()
        
        if not data:
            return pd.DataFrame()
        
        df = pd.DataFrame(data)
        print(f"âœ… êµ¬ê¸€ ì‹œíŠ¸ì—ì„œ ì§ì ‘ ë¡œë“œ ì„±ê³µ: {len(df)}í–‰, {len(df.columns)}ì—´")
        return df
    except Exception as e:
        print(f"âŒ êµ¬ê¸€ ì‹œíŠ¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return pd.DataFrame()

def load_data_from_google_sheets_cached(force_refresh=False, max_age_hours=1):
    """
    ìºì‹±ëœ êµ¬ê¸€ ì‹œíŠ¸ ë°ì´í„° ë¡œë“œ (ì¦ë¶„ ì—…ë°ì´íŠ¸ ì§€ì›)
    
    Args:
        force_refresh: ê°•ì œ ìƒˆë¡œê³ ì¹¨ ì—¬ë¶€
        max_age_hours: ìºì‹œ ìœ íš¨ ì‹œê°„ (ì‹œê°„)
    
    Returns:
        DataFrame: êµ¬ê¸€ ì‹œíŠ¸ ë°ì´í„°
    """
    try:
        # ê°•ì œ ìƒˆë¡œê³ ì¹¨ì´ ì•„ë‹Œ ê²½ìš° ìºì‹œ í™•ì¸
        if not force_refresh:
            cached_df, cached_metadata = load_sheets_cache()
            if cached_df is not None and is_cache_valid(cached_metadata, max_age_hours):
                print("âœ… ìœ íš¨í•œ ìºì‹œ ë°ì´í„° ì‚¬ìš©")
                return cached_df
        
        # ìºì‹œê°€ ì—†ê±°ë‚˜ ë§Œë£Œëœ ê²½ìš° ìƒˆë¡œ ë¡œë“œ
        print("ğŸ”„ êµ¬ê¸€ ì‹œíŠ¸ì—ì„œ ìƒˆ ë°ì´í„° ë¡œë“œ ì¤‘...")
        new_df = load_data_from_google_sheets()
        
        if new_df.empty:
            # ìƒˆ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨ì‹œ ìºì‹œ ë°ì´í„°ë¼ë„ ë°˜í™˜
            cached_df, _ = load_sheets_cache()
            if cached_df is not None:
                st.warning("âš ï¸ ìƒˆ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨, ìºì‹œ ë°ì´í„° ì‚¬ìš©")
                return cached_df
            return pd.DataFrame()
        
        # ìƒˆ ë°ì´í„°ë¥¼ ìºì‹œì— ì €ì¥
        metadata = {
            'timestamp': time.time(),
            'row_count': len(new_df),
            'column_count': len(new_df.columns),
            'last_updated': datetime.now().isoformat()
        }
        
        save_sheets_cache(new_df, metadata)
        return new_df
        
    except Exception as e:
        print(f"âŒ ìºì‹±ëœ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        # ì˜¤ë¥˜ ë°œìƒì‹œ ìºì‹œ ë°ì´í„°ë¼ë„ ë°˜í™˜
        cached_df, _ = load_sheets_cache()
        if cached_df is not None:
            st.error(f"ë°ì´í„° ë¡œë“œ ì˜¤ë¥˜ ë°œìƒ, ìºì‹œ ë°ì´í„° ì‚¬ìš©: {e}")
            return cached_df
        return pd.DataFrame()

def test_connection():
    """ì—°ê²° í…ŒìŠ¤íŠ¸"""
    status = get_connection_status()
    return status['connected']

# ================================
# ê°€ì¤‘ì¹˜ ê³„ì‚° í•¨ìˆ˜ë“¤ (í¸ì°¨ ì¶•ì†Œ & ë‹¤ì–‘ì„± í™•ë³´)
# ================================

def get_major_media_list():
    """ì¤‘ìš” ë§¤ì²´ ëª©ë¡ ì •ì˜"""
    return {
        'ì¡°ì„ ì¼ë³´', 'ë™ì•„ì¼ë³´', 'ì¤‘ì•™ì¼ë³´', 'í•œêµ­ê²½ì œ', 'ë§¤ì¼ê²½ì œ', 'ì„œìš¸ê²½ì œ',
        'KBS', 'MBC', 'SBS', 'YTN', 'JTBC',
        'í•œêµ­ê²½ì œì‹ ë¬¸', 'íŒŒì´ë‚¸ì…œë‰´ìŠ¤', 'ì´ë°ì¼ë¦¬', 'ë‰´ì‹œìŠ¤', 'ì—°í•©ë‰´ìŠ¤'
    }

def calculate_media_weight(media_name):
    """ë§¤ì²´ë³„ ê°€ì¤‘ì¹˜ ê³„ì‚° (í¸ì°¨ ì¶•ì†Œ ë²„ì „)"""
    if pd.isna(media_name):
        return 0.05
    
    media_name = str(media_name).strip()
    major_media = get_major_media_list()
    
    # ì£¼ìš” ì¼ê°„ì§€
    if media_name in ['ì¡°ì„ ì¼ë³´', 'ë™ì•„ì¼ë³´', 'ì¤‘ì•™ì¼ë³´', 'í•œêµ­ê²½ì œ', 'ë§¤ì¼ê²½ì œ']:
        return 0.12
    
    # ì§€ìƒíŒŒ/ì¢…í¸
    elif media_name in ['KBS', 'MBC', 'SBS', 'YTN', 'JTBC']:
        return 0.10
    
    # ê¸°íƒ€ ì£¼ìš” ë§¤ì²´
    elif media_name in major_media:
        return 0.08
    
    # ì¼ë°˜ ë§¤ì²´
    else:
        return 0.05

def calculate_time_weight(date_str):
    """ì‹œê°„ ê°€ì¤‘ì¹˜ ê³„ì‚° (ìµœì‹ ì¼ìˆ˜ë¡ ë†’ì€ ê°€ì¤‘ì¹˜)"""
    try:
        if pd.isna(date_str):
            return 0.5
        
        date_str = str(date_str).strip()
        
        # í˜„ì¬ ë‚ ì§œ ì •ë³´
        now = datetime.now()
        current_month = now.month
        current_day = now.day
        
        # ì›”ì¼ íŒ¨í„´ ì¶”ì¶œ
        month_day_pattern = r'(\d{1,2})ì›”(\d{1,2})ì¼'
        match = re.search(month_day_pattern, date_str)
        
        if match:
            month = int(match.group(1))
            day = int(match.group(2))
            
            # ì›” ì°¨ì´ ê³„ì‚°
            month_diff = abs(current_month - month)
            if month_diff > 6:  # ì—°ë„ ì°¨ì´ ê³ ë ¤
                month_diff = 12 - month_diff
            
            # ê°€ì¤‘ì¹˜ ê³„ì‚° (ìµœì‹ ì¼ìˆ˜ë¡ ë†’ìŒ)
            if month_diff == 0:
                # ê°™ì€ ë‹¬ì´ë©´ ì¼ ì°¨ì´ ê³ ë ¤
                day_diff = abs(current_day - day)
                if day_diff <= 3:
                    return 1.0  # ìµœê·¼ 3ì¼
                elif day_diff <= 7:
                    return 0.9  # ìµœê·¼ 1ì£¼
                elif day_diff <= 14:
                    return 0.8  # ìµœê·¼ 2ì£¼
                else:
                    return 0.7  # ê°™ì€ ë‹¬
            elif month_diff == 1:
                return 0.6  # ì§€ë‚œ ë‹¬
            elif month_diff == 2:
                return 0.5  # 2ë‹¬ ì „
            else:
                return 0.4  # ê·¸ ì´ì „
        
        return 0.5  # ê¸°ë³¸ê°’
    except:
        return 0.5

def calculate_article_weight(row):
    """ê¸°ì‚¬ ê°€ì¤‘ì¹˜ ê³„ì‚° (í¸ì°¨ ì¶•ì†Œ ë²„ì „)"""
    try:
        weight = 0.0
        
        # 1. êµ¬ë¶„(ì¼ë°˜/ë‹¨ë…) ê°€ì¤‘ì¹˜
        distinction = row.get('êµ¬ë¶„(ì¼ë°˜/ë‹¨ë…)', '')
        if pd.notna(distinction):
            distinction = str(distinction).strip()
            if distinction == 'ë‹¨ë…':
                weight += 0.15
            else:
                weight += 0.08
        else:
            weight += 0.08
        
        # 2. ë§¤ì²´ ê°€ì¤‘ì¹˜
        media = row.get('ë§¤ì²´', '')
        if pd.notna(media):
            weight += calculate_media_weight(media)
        else:
            weight += 0.05
        
        # 3. ì§€ë©´ ê°€ì¤‘ì¹˜
        page = row.get('ì§€ë©´', '')
        if pd.notna(page):
            page = str(page).strip()
            if page == 'ì˜¨ë¼ì¸' or page == '':
                weight += 0.03
            else:
                weight += 0.08
            
            # ì¶”ê°€ ì„¸ë¶€ ìœ„ì¹˜ ê°€ì¤‘ì¹˜ (í¸ì°¨ ì¶•ì†Œ)
            page_lower = page.lower()
            extra_weight = 0.0
            
            if 'TOP' in page.upper():
                extra_weight += 0.05
            elif '1ë‹¨' in page:
                extra_weight += 0.03
            
            # ë©´ìˆ˜ ë¶„ì„
            if 'ë©´' in page_lower:
                page_part = page_lower.split('ë©´')[0]
                # ìˆ«ì ì¶”ì¶œ
                numbers = re.findall(r'\d+', page_part)
                if numbers:
                    page_num = int(numbers[0])
                    # ì•ë©´ì¼ìˆ˜ë¡ ë†’ì€ ê°€ì¤‘ì¹˜ (ìµœëŒ€ 0.12)
                    extra_weight += max(0.0, 0.12 - (page_num - 1) * 0.015)
            
            # ìµœëŒ€ ì¶”ê°€ ê°€ì¤‘ì¹˜ ì œí•œ
            weight += min(extra_weight, 0.2)
        else:
            weight += 0.03
        
        # 4. ì‹œê°„ ê°€ì¤‘ì¹˜ ì¶”ê°€
        date_str = row.get('ë³´ë„ë‚ ì§œ', '')
        time_weight = calculate_time_weight(date_str)
        weight += time_weight * 0.2  # 0.2ë°° ë°˜ì˜
        
        # ìµœì¢… ê°€ì¤‘ì¹˜ ë²”ìœ„ ì œí•œ (0.2 ~ 0.8)
        normalized_weight = min(max(weight, 0.2), 0.8)
        return round(normalized_weight, 3)
    
    except Exception as e:
        print(f"ê°€ì¤‘ì¹˜ ê³„ì‚° ì˜¤ë¥˜: {e}")
        return 0.5

# ================================
# konlpy ê¸°ë°˜ ì—…ê³„ ë§¤í•‘ í•¨ìˆ˜ë“¤ (í™•ì¥)
# ================================

def load_industry_mapping_advanced():
    """í™•ì¥ëœ ì—…ê³„ ë§¤í•‘ ë¡œë“œ"""
    company_to_industry = {
        # ìë™ì°¨
        'í˜„ëŒ€ì°¨': 'ìë™ì°¨', 'í˜„ëŒ€ìë™ì°¨': 'ìë™ì°¨', 'ê¸°ì•„': 'ìë™ì°¨', 'ê¸°ì•„ì°¨': 'ìë™ì°¨',
        'ì‚¼ì„±SDI': 'ìë™ì°¨', 'í•œì˜¨ì‹œìŠ¤í…œ': 'ìë™ì°¨', 'í˜„ëŒ€ëª¨ë¹„ìŠ¤': 'ìë™ì°¨',
        'í…ŒìŠ¬ë¼': 'ìë™ì°¨', 'BMW': 'ìë™ì°¨', 'ë²¤ì¸ ': 'ìë™ì°¨', 'ì•„ìš°ë””': 'ìë™ì°¨',
        'í¬ë“œ': 'ìë™ì°¨', 'ë„ìš”íƒ€': 'ìë™ì°¨', 'í˜¼ë‹¤': 'ìë™ì°¨', 'ë‹›ì‚°': 'ìë™ì°¨',
        'ë¥´ë…¸': 'ìë™ì°¨', 'ë³¼ë³´': 'ìë™ì°¨', 'ì¬ê·œì–´': 'ìë™ì°¨', 'ëœë“œë¡œë²„': 'ìë™ì°¨',
        'ì „ê¸°ì°¨': 'ìë™ì°¨', 'ììœ¨ì£¼í–‰': 'ìë™ì°¨', 'ë°°í„°ë¦¬': 'ìë™ì°¨', 'ëª¨ë¹Œë¦¬í‹°': 'ìë™ì°¨',
        
        # IT/ì „ì
        'ì‚¼ì„±ì „ì': 'IT/ì „ì', 'ì‚¼ì„±': 'IT/ì „ì', 'LGì „ì': 'IT/ì „ì', 'LG': 'IT/ì „ì',
        'ë„¤ì´ë²„': 'IT/ì „ì', 'ì¹´ì¹´ì˜¤': 'IT/ì „ì', 'êµ¬ê¸€': 'IT/ì „ì', 'ì• í”Œ': 'IT/ì „ì',
        'SKí•˜ì´ë‹‰ìŠ¤': 'IT/ì „ì', 'í•˜ì´ë‹‰ìŠ¤': 'IT/ì „ì', 'ë©”íƒ€': 'IT/ì „ì', 'í˜ì´ìŠ¤ë¶': 'IT/ì „ì',
        'ë§ˆì´í¬ë¡œì†Œí”„íŠ¸': 'IT/ì „ì', 'ì•„ë§ˆì¡´': 'IT/ì „ì', 'ë„·í”Œë¦­ìŠ¤': 'IT/ì „ì',
        'AMD': 'IT/ì „ì', 'ì¸í…”': 'IT/ì „ì', 'ì—”ë¹„ë””ì•„': 'IT/ì „ì', 'í€„ì»´': 'IT/ì „ì',
        'ë°˜ë„ì²´': 'IT/ì „ì', 'ì¸ê³µì§€ëŠ¥': 'IT/ì „ì', 'í´ë¼ìš°ë“œ': 'IT/ì „ì', 'ë””ì§€í„¸': 'IT/ì „ì',
        
        # í†µì‹ 
        'KT': 'í†µì‹ ', 'SKT': 'í†µì‹ ', 'SKí…”ë ˆì½¤': 'í†µì‹ ', 'LGìœ í”ŒëŸ¬ìŠ¤': 'í†µì‹ ',
        'LG U+': 'í†µì‹ ', 'í‹°ëª¨ë°”ì¼': 'í†µì‹ ', 'ë²„ë¼ì´ì¦Œ': 'í†µì‹ ', 'AT&T': 'í†µì‹ ',
        '5G': 'í†µì‹ ', 'í†µì‹ ë§': 'í†µì‹ ', 'ë„¤íŠ¸ì›Œí¬': 'í†µì‹ ',
        
        # ê¸ˆìœµ
        'ì‚¼ì„±ì¦ê¶Œ': 'ê¸ˆìœµ', 'ì‹ í•œì€í–‰': 'ê¸ˆìœµ', 'êµ­ë¯¼ì€í–‰': 'ê¸ˆìœµ', 'ìš°ë¦¬ì€í–‰': 'ê¸ˆìœµ',
        'KB': 'ê¸ˆìœµ', 'í•˜ë‚˜ì€í–‰': 'ê¸ˆìœµ', 'ì‚¼ì„±ìƒëª…': 'ê¸ˆìœµ', 'í•œí™”ìƒëª…': 'ê¸ˆìœµ',
        'êµë³´ìƒëª…': 'ê¸ˆìœµ', 'í˜„ëŒ€í•´ìƒ': 'ê¸ˆìœµ', 'ì‚¼ì„±í™”ì¬': 'ê¸ˆìœµ', 'KBì¦ê¶Œ': 'ê¸ˆìœµ',
        'NHíˆ¬ìì¦ê¶Œ': 'ê¸ˆìœµ', 'ë¯¸ë˜ì—ì…‹': 'ê¸ˆìœµ', 'ëŒ€ì‹ ì¦ê¶Œ': 'ê¸ˆìœµ',
        'í•€í…Œí¬': 'ê¸ˆìœµ', 'ê°€ìƒí™”í': 'ê¸ˆìœµ', 'ë¸”ë¡ì²´ì¸': 'ê¸ˆìœµ', 'íˆ¬ì': 'ê¸ˆìœµ',
        
        # í™”í•™/ì—ë„ˆì§€
        'LGí™”í•™': 'í™”í•™/ì—ë„ˆì§€', 'ì‚¼ì„±í™”í•™': 'í™”í•™/ì—ë„ˆì§€', 'SKì—ë„ˆì§€': 'í™”í•™/ì—ë„ˆì§€',
        'í•œí™”': 'í™”í•™/ì—ë„ˆì§€', 'í¬ìŠ¤ì½”': 'í™”í•™/ì—ë„ˆì§€', 'GSì¹¼í…ìŠ¤': 'í™”í•™/ì—ë„ˆì§€',
        'í˜„ëŒ€ì˜¤ì¼ë±…í¬': 'í™”í•™/ì—ë„ˆì§€', 'S-Oil': 'í™”í•™/ì—ë„ˆì§€', 'ë¡¯ë°ì¼€ë¯¸ì¹¼': 'í™”í•™/ì—ë„ˆì§€',
        'í•œí™”ì†”ë£¨ì…˜': 'í™”í•™/ì—ë„ˆì§€', 'í•œí™”ì¼€ë¯¸ì¹¼': 'í™”í•™/ì—ë„ˆì§€',
        'ì¹œí™˜ê²½': 'í™”í•™/ì—ë„ˆì§€', 'íƒ„ì†Œì¤‘ë¦½': 'í™”í•™/ì—ë„ˆì§€', 'ì¬ìƒì—ë„ˆì§€': 'í™”í•™/ì—ë„ˆì§€',
        'ìˆ˜ì†Œ': 'í™”í•™/ì—ë„ˆì§€', 'íƒœì–‘ê´‘': 'í™”í•™/ì—ë„ˆì§€', 'í’ë ¥': 'í™”í•™/ì—ë„ˆì§€',
        
        # í•­ê³µ/ìš´ì†¡
        'ëŒ€í•œí•­ê³µ': 'í•­ê³µ/ìš´ì†¡', 'ì•„ì‹œì•„ë‚˜': 'í•­ê³µ/ìš´ì†¡', 'ì§„ì—ì–´': 'í•­ê³µ/ìš´ì†¡',
        'CJëŒ€í•œí†µìš´': 'í•­ê³µ/ìš´ì†¡', 'ë¡¯ë°íƒë°°': 'í•­ê³µ/ìš´ì†¡', 'í•œì§„': 'í•­ê³µ/ìš´ì†¡',
        'í˜„ëŒ€ê¸€ë¡œë¹„ìŠ¤': 'í•­ê³µ/ìš´ì†¡', 'íŒ¬ì˜¤ì…˜': 'í•­ê³µ/ìš´ì†¡', 'HMM': 'í•­ê³µ/ìš´ì†¡',
        'ë¬¼ë¥˜': 'í•­ê³µ/ìš´ì†¡', 'ë°°ì†¡': 'í•­ê³µ/ìš´ì†¡', 'ìš´ì†¡': 'í•­ê³µ/ìš´ì†¡',
        
        # ê±´ì„¤/ë¶€ë™ì‚°
        'ì‚¼ì„±ë¬¼ì‚°': 'ê±´ì„¤/ë¶€ë™ì‚°', 'í˜„ëŒ€ê±´ì„¤': 'ê±´ì„¤/ë¶€ë™ì‚°', 'ëŒ€ìš°ê±´ì„¤': 'ê±´ì„¤/ë¶€ë™ì‚°',
        'GSê±´ì„¤': 'ê±´ì„¤/ë¶€ë™ì‚°', 'ë¡¯ë°ê±´ì„¤': 'ê±´ì„¤/ë¶€ë™ì‚°', 'í¬ìŠ¤ì½”ê±´ì„¤': 'ê±´ì„¤/ë¶€ë™ì‚°',
        'SKê±´ì„¤': 'ê±´ì„¤/ë¶€ë™ì‚°', 'í•œí™”ê±´ì„¤': 'ê±´ì„¤/ë¶€ë™ì‚°', 'ëŒ€ë¦¼ê±´ì„¤': 'ê±´ì„¤/ë¶€ë™ì‚°',
        'í˜„ëŒ€ì—”ì§€ë‹ˆì–´ë§': 'ê±´ì„¤/ë¶€ë™ì‚°', 'ì‚¼ì„±ì—”ì§€ë‹ˆì–´ë§': 'ê±´ì„¤/ë¶€ë™ì‚°',
        'ê°œë°œ': 'ê±´ì„¤/ë¶€ë™ì‚°', 'ë¶„ì–‘': 'ê±´ì„¤/ë¶€ë™ì‚°', 'ì¬ê±´ì¶•': 'ê±´ì„¤/ë¶€ë™ì‚°',
        'ë¦¬ëª¨ë¸ë§': 'ê±´ì„¤/ë¶€ë™ì‚°', 'ë¶€ë™ì‚°': 'ê±´ì„¤/ë¶€ë™ì‚°',
        
        # ìœ í†µ/ì†Œë¹„ì¬
        'ë¡¯ë°': 'ìœ í†µ/ì†Œë¹„ì¬', 'ì‹ ì„¸ê³„': 'ìœ í†µ/ì†Œë¹„ì¬', 'í˜„ëŒ€ë°±í™”ì ': 'ìœ í†µ/ì†Œë¹„ì¬',
        'ì´ë§ˆíŠ¸': 'ìœ í†µ/ì†Œë¹„ì¬', 'í™ˆí”ŒëŸ¬ìŠ¤': 'ìœ í†µ/ì†Œë¹„ì¬', 'ì½”ìŠ¤íŠ¸ì½”': 'ìœ í†µ/ì†Œë¹„ì¬',
        'ì¿ íŒ¡': 'ìœ í†µ/ì†Œë¹„ì¬', '11ë²ˆê°€': 'ìœ í†µ/ì†Œë¹„ì¬', 'Gë§ˆì¼“': 'ìœ í†µ/ì†Œë¹„ì¬',
        'ì˜¥ì…˜': 'ìœ í†µ/ì†Œë¹„ì¬', 'ìœ„ë©”í”„': 'ìœ í†µ/ì†Œë¹„ì¬', 'í‹°ëª¬': 'ìœ í†µ/ì†Œë¹„ì¬',
        'ì´ì»¤ë¨¸ìŠ¤': 'ìœ í†µ/ì†Œë¹„ì¬', 'ì˜¨ë¼ì¸': 'ìœ í†µ/ì†Œë¹„ì¬', 'ì†Œë¹„': 'ìœ í†µ/ì†Œë¹„ì¬',
        
        # ì œì•½/ë°”ì´ì˜¤
        'ì‚¼ì„±ë°”ì´ì˜¤ë¡œì§ìŠ¤': 'ì œì•½/ë°”ì´ì˜¤', 'ì…€íŠ¸ë¦¬ì˜¨': 'ì œì•½/ë°”ì´ì˜¤', 'ìœ í•œì–‘í–‰': 'ì œì•½/ë°”ì´ì˜¤',
        'ì¢…ê·¼ë‹¹': 'ì œì•½/ë°”ì´ì˜¤', 'í•œë¯¸ì•½í’ˆ': 'ì œì•½/ë°”ì´ì˜¤', 'ëŒ€ì›…ì œì•½': 'ì œì•½/ë°”ì´ì˜¤',
        'ë…¹ì‹­ì': 'ì œì•½/ë°”ì´ì˜¤', 'SKë°”ì´ì˜¤íŒœ': 'ì œì•½/ë°”ì´ì˜¤', 'ì—ì´ë¹„ì—˜ë°”ì´ì˜¤': 'ì œì•½/ë°”ì´ì˜¤',
        'ì‹ ë¼ì  ': 'ì œì•½/ë°”ì´ì˜¤', 'ë©”ë””í†¡ìŠ¤': 'ì œì•½/ë°”ì´ì˜¤', 'ì œë„¥ì‹ ': 'ì œì•½/ë°”ì´ì˜¤',
        'ë°±ì‹ ': 'ì œì•½/ë°”ì´ì˜¤', 'ì¹˜ë£Œì œ': 'ì œì•½/ë°”ì´ì˜¤', 'ì„ìƒ': 'ì œì•½/ë°”ì´ì˜¤',
        'ì‹ ì•½': 'ì œì•½/ë°”ì´ì˜¤', 'ì˜ë£Œ': 'ì œì•½/ë°”ì´ì˜¤', 'ë°”ì´ì˜¤': 'ì œì•½/ë°”ì´ì˜¤',
        
        # ì—”í„°í…Œì¸ë¨¼íŠ¸/ë¯¸ë””ì–´
        'SM': 'ì—”í„°í…Œì¸ë¨¼íŠ¸/ë¯¸ë””ì–´', 'YG': 'ì—”í„°í…Œì¸ë¨¼íŠ¸/ë¯¸ë””ì–´', 'JYP': 'ì—”í„°í…Œì¸ë¨¼íŠ¸/ë¯¸ë””ì–´',
        'CJ': 'ì—”í„°í…Œì¸ë¨¼íŠ¸/ë¯¸ë””ì–´', 'CJ ENM': 'ì—”í„°í…Œì¸ë¨¼íŠ¸/ë¯¸ë””ì–´', 'JTBC': 'ì—”í„°í…Œì¸ë¨¼íŠ¸/ë¯¸ë””ì–´',
        'tvN': 'ì—”í„°í…Œì¸ë¨¼íŠ¸/ë¯¸ë””ì–´', 'KBS': 'ì—”í„°í…Œì¸ë¨¼íŠ¸/ë¯¸ë””ì–´', 'MBC': 'ì—”í„°í…Œì¸ë¨¼íŠ¸/ë¯¸ë””ì–´',
        'SBS': 'ì—”í„°í…Œì¸ë¨¼íŠ¸/ë¯¸ë””ì–´', 'YTN': 'ì—”í„°í…Œì¸ë¨¼íŠ¸/ë¯¸ë””ì–´',
        'í•˜ì´ë¸Œ': 'ì—”í„°í…Œì¸ë¨¼íŠ¸/ë¯¸ë””ì–´', 'ì™€ì´ì§€': 'ì—”í„°í…Œì¸ë¨¼íŠ¸/ë¯¸ë””ì–´',
        'ì½˜í…ì¸ ': 'ì—”í„°í…Œì¸ë¨¼íŠ¸/ë¯¸ë””ì–´', 'í”Œë«í¼': 'ì—”í„°í…Œì¸ë¨¼íŠ¸/ë¯¸ë””ì–´',
        'ìŠ¤íŠ¸ë¦¬ë°': 'ì—”í„°í…Œì¸ë¨¼íŠ¸/ë¯¸ë””ì–´', 'OTT': 'ì—”í„°í…Œì¸ë¨¼íŠ¸/ë¯¸ë””ì–´',
        
        # ê²Œì„
        'NCSoft': 'ê²Œì„', 'ë„¥ìŠ¨': 'ê²Œì„', 'ì¹´ì¹´ì˜¤ê²Œì„ì¦ˆ': 'ê²Œì„', 'í„ì–´ë¹„ìŠ¤': 'ê²Œì„',
        'í¬ë˜í”„í†¤': 'ê²Œì„', 'ì»´íˆ¬ìŠ¤': 'ê²Œì„', 'ì›¹ì  ': 'ê²Œì„', 'ë„¤ì˜¤ìœ„ì¦ˆ': 'ê²Œì„',
        'ë„¤íŠ¸ë§ˆë¸”': 'ê²Œì„', 'ë‹ˆíŠ¸ë¡œ': 'ê²Œì„', 'ë“œë˜ê³¤í”Œë¼ì´': 'ê²Œì„',
        'ë©”íƒ€ë²„ìŠ¤': 'ê²Œì„', 'ê²Œì„': 'ê²Œì„', 'ëª¨ë°”ì¼ê²Œì„': 'ê²Œì„', 'PCê²Œì„': 'ê²Œì„',
        
        # ì‹í’ˆ/ìŒë£Œ
        'ë†ì‹¬': 'ì‹í’ˆ/ìŒë£Œ', 'ì˜¤ë¦¬ì˜¨': 'ì‹í’ˆ/ìŒë£Œ', 'ë¡¯ë°ì œê³¼': 'ì‹í’ˆ/ìŒë£Œ',
        'í•´íƒœ': 'ì‹í’ˆ/ìŒë£Œ', 'ë¹™ê·¸ë ˆ': 'ì‹í’ˆ/ìŒë£Œ', 'ë§¤ì¼ìœ ì—…': 'ì‹í’ˆ/ìŒë£Œ',
        'ë‚¨ì–‘ìœ ì—…': 'ì‹í’ˆ/ìŒë£Œ', 'ë™ì›F&B': 'ì‹í’ˆ/ìŒë£Œ', 'CJì œì¼ì œë‹¹': 'ì‹í’ˆ/ìŒë£Œ',
        'ë¡¯ë°ì¹ ì„±': 'ì‹í’ˆ/ìŒë£Œ', 'ì½”ì¹´ì½œë¼': 'ì‹í’ˆ/ìŒë£Œ', 'í©ì‹œ': 'ì‹í’ˆ/ìŒë£Œ',
        'ê±´ê°•': 'ì‹í’ˆ/ìŒë£Œ', 'í”„ë¦¬ë¯¸ì—„': 'ì‹í’ˆ/ìŒë£Œ', 'ì¹œí™˜ê²½': 'ì‹í’ˆ/ìŒë£Œ',
        'ìœ ê¸°ë†': 'ì‹í’ˆ/ìŒë£Œ', 'ê°€ê³µì‹í’ˆ': 'ì‹í’ˆ/ìŒë£Œ', 'ìŒë£Œ': 'ì‹í’ˆ/ìŒë£Œ',
        
        # ìŠ¤í¬ì¸ /ë ˆì €
        'ë‚˜ì´í‚¤': 'ìŠ¤í¬ì¸ /ë ˆì €', 'ì•„ë””ë‹¤ìŠ¤': 'ìŠ¤í¬ì¸ /ë ˆì €', 'í‘¸ë§ˆ': 'ìŠ¤í¬ì¸ /ë ˆì €',
        'ìŠ¤í¬ì¸ ': 'ìŠ¤í¬ì¸ /ë ˆì €', 'ìš´ë™': 'ìŠ¤í¬ì¸ /ë ˆì €', 'í—¬ìŠ¤': 'ìŠ¤í¬ì¸ /ë ˆì €',
        'ë ˆì €': 'ìŠ¤í¬ì¸ /ë ˆì €', 'ê³¨í”„': 'ìŠ¤í¬ì¸ /ë ˆì €', 'ì¶•êµ¬': 'ìŠ¤í¬ì¸ /ë ˆì €',
        'ì•¼êµ¬': 'ìŠ¤í¬ì¸ /ë ˆì €', 'ë†êµ¬': 'ìŠ¤í¬ì¸ /ë ˆì €', 'í…Œë‹ˆìŠ¤': 'ìŠ¤í¬ì¸ /ë ˆì €',
        
        # êµìœ¡
        'êµìœ¡': 'êµìœ¡', 'ì—ë“€í…Œí¬': 'êµìœ¡', 'ì˜¨ë¼ì¸êµìœ¡': 'êµìœ¡', 'í•™ì›': 'êµìœ¡',
        'ëŒ€í•™': 'êµìœ¡', 'í•™êµ': 'êµìœ¡', 'ìˆ˜ì—…': 'êµìœ¡', 'ê°•ì˜': 'êµìœ¡',
        
        # ì •ë¶€/ê³µê³µê¸°ê´€
        'ì •ë¶€': 'ì •ë¶€/ê³µê³µ', 'êµ­ì •ì›': 'ì •ë¶€/ê³µê³µ', 'ê²½ì°°': 'ì •ë¶€/ê³µê³µ',
        'ì†Œë°©ì„œ': 'ì •ë¶€/ê³µê³µ', 'ê³µê³µê¸°ê´€': 'ì •ë¶€/ê³µê³µ', 'ì§€ìì²´': 'ì •ë¶€/ê³µê³µ',
        'ì‹œì²­': 'ì •ë¶€/ê³µê³µ', 'êµ¬ì²­': 'ì •ë¶€/ê³µê³µ', 'ë„ì²­': 'ì •ë¶€/ê³µê³µ',
        
        # ê¸°íƒ€
        'ê¸°íƒ€': 'ê¸°íƒ€', 'ì¼ë°˜': 'ê¸°íƒ€', 'ì‚¬íšŒ': 'ê¸°íƒ€', 'ë¬¸í™”': 'ê¸°íƒ€',
        'ì¢…êµ': 'ê¸°íƒ€', 'í™˜ê²½': 'ê¸°íƒ€', 'ì‚¬ê±´': 'ê¸°íƒ€', 'ì‚¬ê³ ': 'ê¸°íƒ€'
    }
    
    print(f"âœ… ì´ {len(company_to_industry)}ê°œ ê¸°ì—…-ì—…ê³„ ë§¤í•‘ ë¡œë“œ ì™„ë£Œ")
    return company_to_industry

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ê¸°ì—…ëª… ì¶”ì¶œ: soynlp â†’ konlpy Okt â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def extract_company_names_okt(title, content=""):
    """konlpy Oktë¥¼ ì‚¬ìš©í•œ ê¸°ì—…ëª… ì¶”ì¶œ"""
    try:
        full_text = f"{title} {content}".strip()
        if not full_text:
            return []
        
        # ëª…ì‚¬ ì¶”ì¶œ
        nouns = okt.nouns(full_text)
        
        # ê¸°ì—…ëª… í›„ë³´ ì¶”ì¶œ
        companies = set()
        
        # ê¸°ì—…ëª… íŒ¨í„´ ë§¤ì¹­
        company_patterns = [
            r'([ê°€-í£]{2,8}(?:ì „ì|ìë™ì°¨|ê±´ì„¤|í™”í•™|ì œì•½|í†µì‹ |ê¸ˆìœµ|ë³´í—˜|ì€í–‰|ì¹´ë“œ|ì¦ê¶Œ|íˆ¬ì|ê·¸ë£¹|í™€ë”©ìŠ¤|ë°”ì´ì˜¤|ê²Œì„|ì—”í„°|ë¯¸ë””ì–´))',
            r'([ê°€-í£]{2,6}(?:ì£¼ì‹íšŒì‚¬|ãˆœ|íšŒì‚¬|ê¸°ì—…|ì‚°ì—…|ë¬¼ì‚°|ìƒì‚¬|ê°œë°œ|í…Œí¬|ì†Œí”„íŠ¸|ì‹œìŠ¤í…œ))',
            r'(í˜„ëŒ€[ê°€-í£]{0,4}|ì‚¼ì„±[ê°€-í£]{0,4}|LG[ê°€-í£]{0,4}|SK[ê°€-í£]{0,4}|ë¡¯ë°[ê°€-í£]{0,4})',
            r'(GS[ê°€-í£]{0,4}|CJ[ê°€-í£]{0,4}|í•œí™”[ê°€-í£]{0,4}|í¬ìŠ¤ì½”[ê°€-í£]{0,4})',
            r'([A-Z]{2,6}[ê°€-í£]{0,4})',  # ì˜ë¬¸+í•œê¸€ ì¡°í•©
            r'([ê°€-í£]{2,6}[A-Z]{1,4})',  # í•œê¸€+ì˜ë¬¸ ì¡°í•©
        ]
        
        for pattern in company_patterns:
            matches = re.findall(pattern, full_text)
            companies.update(matches)
        
        # ëª…ì‚¬ì—ì„œ ê¸°ì—…ëª… ì°¾ê¸°
        for noun in nouns:
            if (len(noun) >= 2 and len(noun) <= 8 and
                re.search(r'[ê°€-í£]', noun) and
                not noun.isdigit()):
                companies.add(noun)
        
        # ë¶ˆìš©ì–´ ì œê±°
        stopwords = {
            'ê¸°ì', 'ì‚¬ì§„', 'ì œê³µ', 'ê´€ë ¨', 'ì—…ê³„', 'ì‹œì¥', 'ì‚°ì—…', 'ë¶„ì•¼', 'íšŒì‚¬', 'ê¸°ì—…',
            'ë°œí‘œ', 'ê³µê°œ', 'ì„¤ëª…', 'ë§í–ˆë‹¤', 'ë°í˜”ë‹¤', 'ì „í–ˆë‹¤', 'ë³´ë„', 'ë‰´ìŠ¤', 'ê¸°ì‚¬',
            'ëŒ€í‘œ', 'ì‚¬ì¥', 'íšŒì¥', 'ë¶€ì‚¬ì¥', 'ì´ì‚¬', 'ìƒë¬´', 'ì „ë¬´', 'ë³¸ë¶€ì¥', 'íŒ€ì¥',
            'ì˜¤ëŠ˜', 'ì–´ì œ', 'ë‚´ì¼', 'ìµœê·¼', 'í˜„ì¬', 'ë‹¹ì‹œ', 'ì´ë²ˆ', 'ë‹¤ìŒ', 'ì§€ë‚œ'
        }
        
        # ìµœì¢… ê¸°ì—…ëª… í•„í„°ë§
        final_companies = []
        for company in companies:
            if (company not in stopwords and
                len(company) >= 2 and len(company) <= 10 and
                not company.isdigit() and
                re.search(r'[ê°€-í£]', company)):
                final_companies.append(company)
        
        return final_companies
        
    except Exception as e:
        print(f"konlpy ê¸°ì—…ëª… ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        return extract_company_names_simple(title, content)

def extract_company_names_simple(title, content=""):
    """ê°„ë‹¨í•œ ê¸°ì—…ëª… ì¶”ì¶œ (ë°±ì—…ìš©)"""
    full_text = f"{title} {content}".strip()
    if not full_text:
        return []
    
    # ê¸°ì—…ëª… íŒ¨í„´ë“¤
    company_patterns = [
        r'([ê°€-í£]{2,8}(?:ì „ì|ìë™ì°¨|ê±´ì„¤|í™”í•™|ì œì•½|í†µì‹ |ê¸ˆìœµ|ë³´í—˜|ì€í–‰|ì¹´ë“œ|ì¦ê¶Œ|íˆ¬ì|ê·¸ë£¹|í™€ë”©ìŠ¤|ë°”ì´ì˜¤|ê²Œì„|ì—”í„°|ë¯¸ë””ì–´))',
        r'([ê°€-í£]{2,6}(?:ì£¼ì‹íšŒì‚¬|ãˆœ|íšŒì‚¬|ê¸°ì—…|ì‚°ì—…|ë¬¼ì‚°|ìƒì‚¬|ê°œë°œ|í…Œí¬|ì†Œí”„íŠ¸|ì‹œìŠ¤í…œ))',
        r'(í˜„ëŒ€[ê°€-í£]{0,4}|ì‚¼ì„±[ê°€-í£]{0,4}|LG[ê°€-í£]{0,4}|SK[ê°€-í£]{0,4}|ë¡¯ë°[ê°€-í£]{0,4})',
        r'(GS[ê°€-í£]{0,4}|CJ[ê°€-í£]{0,4}|í•œí™”[ê°€-í£]{0,4}|í¬ìŠ¤ì½”[ê°€-í£]{0,4})',
        r'([A-Z]{2,6}[ê°€-í£]{0,4})',  # ì˜ë¬¸+í•œê¸€ ì¡°í•©
        r'([ê°€-í£]{2,6}[A-Z]{1,4})',  # í•œê¸€+ì˜ë¬¸ ì¡°í•©
    ]
    
    companies = set()
    
    # íŒ¨í„´ ë§¤ì¹­
    for pattern in company_patterns:
        matches = re.findall(pattern, full_text)
        companies.update(matches)
    
    # ë‹¨ì–´ ê¸°ë°˜ í•„í„°ë§
    words = full_text.split()
    stopwords = {
        'ê¸°ì', 'ì‚¬ì§„', 'ì œê³µ', 'ê´€ë ¨', 'ì—…ê³„', 'ì‹œì¥', 'ì‚°ì—…', 'ë¶„ì•¼', 'íšŒì‚¬', 'ê¸°ì—…',
        'ë°œí‘œ', 'ê³µê°œ', 'ì„¤ëª…', 'ë§í–ˆë‹¤', 'ë°í˜”ë‹¤', 'ì „í–ˆë‹¤', 'ë³´ë„', 'ë‰´ìŠ¤', 'ê¸°ì‚¬',
        'ëŒ€í‘œ', 'ì‚¬ì¥', 'íšŒì¥', 'ë¶€ì‚¬ì¥', 'ì´ì‚¬', 'ìƒë¬´', 'ì „ë¬´', 'ë³¸ë¶€ì¥', 'íŒ€ì¥'
    }
    
    for word in words:
        word = word.strip()
        if (len(word) >= 2 and len(word) <= 10 and
            word not in stopwords and
            not word.isdigit() and
            re.search(r'[ê°€-í£]', word)):
            companies.add(word)
    
    return list(companies)

def map_industry_advanced(row, company_to_industry):
    """ê³ ê¸‰ ì—…ê³„ ë§¤í•‘ (konlpy í™œìš©)"""
    # ì œëª© ì°¾ê¸°
    title_columns = ['ì œëª©', 'title', 'í—¤ë“œë¼ì¸', 'ê¸°ì‚¬ì œëª©', 'ê¸°ì‚¬ ì œëª©']
    title = ""
    for col in title_columns:
        if col in row and pd.notna(row[col]):
            title = str(row[col]).strip()
            break
    
    # ë‚´ìš© ì°¾ê¸°
    content_columns = ['ë‚´ìš©', 'content', 'ë³¸ë¬¸', 'ê¸°ì‚¬ë‚´ìš©', 'ì£¼ìš”ë‚´ìš©']
    content = ""
    for col in content_columns:
        if col in row and pd.notna(row[col]):
            content = str(row[col]).strip()
            break
    
    if not title:
        return 'ê¸°íƒ€'
    
    # konlpy ê¸°ë°˜ ê¸°ì—…ëª… ì¶”ì¶œ
    companies = extract_company_names_okt(title, content)
    
    if not companies:
        return 'ê¸°íƒ€'
    
    # ì—…ê³„ ë§¤í•‘
    matched_industries = []
    for company in companies:
        # ì •í™•í•œ ë§¤ì¹­
        if company in company_to_industry:
            matched_industries.append(company_to_industry[company])
            continue
        
        # ë¶€ë¶„ ë§¤ì¹­
        for mapped_company, industry in company_to_industry.items():
            if company in mapped_company or mapped_company in company:
                matched_industries.append(industry)
                break
    
    if matched_industries:
        # ê°€ì¥ ë§ì´ ë§¤ì¹­ëœ ì—…ê³„ ë°˜í™˜
        industry_counts = Counter(matched_industries)
        return industry_counts.most_common(1)[0][0]
    
    return 'ê¸°íƒ€'

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì—…ê³„ë³„ í‚¤ì›Œë“œ ì¶”ì¶œ: soynlp â†’ konlpy Okt â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def extract_industry_keywords(df, industry):
    """ì—…ê³„ë³„ íŠ¹í™” í‚¤ì›Œë“œ ì¶”ì¶œ (konlpy ì‚¬ìš©)"""
    if industry != 'ì „ì²´':
        industry_articles = df[df['ì—…ê³„'] == industry]
    else:
        industry_articles = df
    
    titles = industry_articles['ì œëª©'].dropna().tolist()
    
    # konlpy Oktë¡œ ì—…ê³„ íŠ¹í™” í‚¤ì›Œë“œ ì¶”ì¶œ
    if not titles:
        return []
    
    # ì „ì²´ í…ìŠ¤íŠ¸ ê²°í•©
    all_text = " ".join(titles)
    
    # ëª…ì‚¬ ì¶”ì¶œ
    nouns = okt.nouns(all_text)
    freq = Counter(nouns)
    
    # ë¶ˆìš©ì–´ ì œê±°
    stopwords = {
        'ê¸°ì', 'ì‚¬ì§„', 'ì œê³µ', 'ê´€ë ¨', 'ì—…ê³„', 'ì‹œì¥', 'ë¶„ì•¼', 'íšŒì‚¬', 'ê¸°ì—…',
        'ë°œí‘œ', 'ê³µê°œ', 'ì„¤ëª…', 'ë³´ë„', 'ë‰´ìŠ¤', 'ê¸°ì‚¬', 'ì˜¤ëŠ˜', 'ì–´ì œ', 'ë‚´ì¼'
    }
    
    keywords = [(w, freq[w]) for w in freq if len(w) > 1 and w not in stopwords]
    keywords.sort(key=lambda x: x[1], reverse=True)
    
    # ì—…ê³„ë³„ ê°€ì¤‘ì¹˜ ì¡°ì •
    industry_weights = {
        'ìë™ì°¨': ['ì „ê¸°ì°¨', 'ììœ¨ì£¼í–‰', 'ë°°í„°ë¦¬', 'ëª¨ë¹Œë¦¬í‹°', 'ì „ë™í™”'],
        'IT/ì „ì': ['AI', 'ë°˜ë„ì²´', 'í´ë¼ìš°ë“œ', 'ë””ì§€í„¸', 'ì¸ê³µì§€ëŠ¥'],
        'ê¸ˆìœµ': ['í•€í…Œí¬', 'ê°€ìƒí™”í', 'ë¸”ë¡ì²´ì¸', 'íˆ¬ì', 'ë””ì§€í„¸ë±…í‚¹'],
        'ë°”ì´ì˜¤': ['ë°±ì‹ ', 'ì¹˜ë£Œì œ', 'ì„ìƒ', 'ì‹ ì•½', 'ì˜ë£Œ'],
        'í™”í•™/ì—ë„ˆì§€': ['ì¹œí™˜ê²½', 'íƒ„ì†Œì¤‘ë¦½', 'ì¬ìƒì—ë„ˆì§€', 'ìˆ˜ì†Œ'],
        'í•­ê³µ/ìš´ì†¡': ['ë¬¼ë¥˜', 'ë°°ì†¡', 'ëª¨ë¹Œë¦¬í‹°', 'ìš´ì†¡'],
        'ê±´ì„¤/ë¶€ë™ì‚°': ['ê°œë°œ', 'ë¶„ì–‘', 'ì¬ê±´ì¶•', 'ë¦¬ëª¨ë¸ë§'],
        'ìœ í†µ/ì†Œë¹„ì¬': ['ì´ì»¤ë¨¸ìŠ¤', 'ì˜¨ë¼ì¸', 'ë°°ì†¡', 'ì†Œë¹„'],
        'ì—”í„°í…Œì¸ë¨¼íŠ¸/ë¯¸ë””ì–´': ['ì½˜í…ì¸ ', 'í”Œë«í¼', 'ìŠ¤íŠ¸ë¦¬ë°', 'OTT'],
        'ê²Œì„': ['ë©”íƒ€ë²„ìŠ¤', 'ê²Œì„', 'í”Œë«í¼', 'ëª¨ë°”ì¼'],
        'ì‹í’ˆ/ìŒë£Œ': ['ê±´ê°•', 'í”„ë¦¬ë¯¸ì—„', 'ì¹œí™˜ê²½', 'ìœ ê¸°ë†']
    }
    
    if industry in industry_weights:
        boost_keywords = industry_weights[industry]
        enhanced_keywords = []
        for word, score in keywords:
            if any(boost in word for boost in boost_keywords):
                enhanced_keywords.append((word, score * 1.5))
            else:
                enhanced_keywords.append((word, score))
        return sorted(enhanced_keywords, key=lambda x: x[1], reverse=True)[:20]
    
    return keywords[:20]

def add_industry_column(df):
    """DataFrameì— ì—…ê³„ ì»¬ëŸ¼ ì¶”ê°€"""
    if df is None or df.empty:
        return df
    
    print("ğŸš€ ì—…ê³„ ë§¤í•‘ ì‹œì‘...")
    
    # ì—…ê³„ ë§¤í•‘ ë°ì´í„° ë¡œë“œ
    company_to_industry = load_industry_mapping_advanced()
    
    if not company_to_industry:
        print("âŒ ì—…ê³„ ë§¤í•‘ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return df
    
    # ì—…ê³„ ì»¬ëŸ¼ ì´ˆê¸°í™”
    if 'ì—…ê³„' not in df.columns:
        df['ì—…ê³„'] = 'ê¸°íƒ€'
    
    total_rows = len(df)
    mapped_count = 0
    
    # ê° í–‰ì— ëŒ€í•´ ì—…ê³„ ë§¤í•‘
    for idx, row in df.iterrows():
        if pd.isna(df.loc[idx, 'ì—…ê³„']) or df.loc[idx, 'ì—…ê³„'] == 'ê¸°íƒ€':
            industry = map_industry_advanced(row, company_to_industry)
            df.loc[idx, 'ì—…ê³„'] = industry
            if industry != 'ê¸°íƒ€':
                mapped_count += 1
    
    # ê²°ê³¼ ì¶œë ¥
    industry_counts = df['ì—…ê³„'].value_counts()
    print(f"âœ… ì—…ê³„ ë§¤í•‘ ì™„ë£Œ!")
    print(f"ğŸ“Š ì´ {mapped_count}/{total_rows}ê°œ ê¸°ì‚¬ ë§¤í•‘ ì„±ê³µ")
    print(f"ğŸ“‹ ì—…ê³„ë³„ ë¶„í¬:")
    for industry, count in industry_counts.head(10).items():
        print(f"  - {industry}: {count}ê°œ")
    
    return df

# ================================
# ë‚ ì§œ ì²˜ë¦¬ í•¨ìˆ˜ë“¤
# ================================

def extract_month_from_date(date_str):
    """ë‚ ì§œì—ì„œ ì›” ì¶”ì¶œ"""
    if pd.isnull(date_str):
        return None
    
    date_str = str(date_str).strip()
    
    # 1ì›”1ì¼ í˜•ì‹ ì²˜ë¦¬
    month_day_pattern = r'(\d{1,2})ì›”(\d{1,2})ì¼'
    match = re.search(month_day_pattern, date_str)
    
    if match:
        month = int(match.group(1))
        return f"{month}ì›”"
    
    return None

def extract_week_from_date(date_str):
    """ë‚ ì§œì—ì„œ ì£¼ì°¨ ì¶”ì¶œ"""
    if pd.isnull(date_str):
        return None
    
    date_str = str(date_str).strip()
    
    # 1ì›”1ì¼ í˜•ì‹ ì²˜ë¦¬
    month_day_pattern = r'(\d{1,2})ì›”(\d{1,2})ì¼'
    match = re.search(month_day_pattern, date_str)
    
    if match:
        month = int(match.group(1))
        day = int(match.group(2))
        
        # ì£¼ì°¨ ê³„ì‚°
        if day <= 7:
            week = 1
        elif day <= 14:
            week = 2
        elif day <= 21:
            week = 3
        elif day <= 28:
            week = 4
        else:
            week = 5
        
        return f"{month}ì›”{week}ì£¼ì°¨"
    
    return None

# ================================
# ë„¤ì´ë²„ ë§í¬ í•˜ì´í¼ë§í¬ ì²˜ë¦¬ í•¨ìˆ˜ë“¤
# ================================

def get_naver_link_column(df):
    """êµ¬ê¸€ ì‹œíŠ¸ì—ì„œ ë„¤ì´ë²„ë§í¬ ì»¬ëŸ¼ëª…ì„ ì°¾ëŠ” í•¨ìˆ˜"""
    possible_columns = ['ë„¤ì´ë²„ë§í¬', 'ë„¤ì´ë²„ ë§í¬', 'ë„¤ì´ë²„URL', 'ë„¤ì´ë²„ URL', 'naver_link', 'naver_url']
    
    for col in possible_columns:
        if col in df.columns:
            return col
    
    # ìœ ì‚¬í•œ ì»¬ëŸ¼ëª… ì°¾ê¸°
    for col in df.columns:
        if 'ë„¤ì´ë²„' in str(col) and ('ë§í¬' in str(col) or 'URL' in str(col)):
            return col
    
    return None

def get_media_link_column(df):
    """êµ¬ê¸€ ì‹œíŠ¸ì—ì„œ ë§¤ì²´ë§í¬ ì»¬ëŸ¼ëª…ì„ ì°¾ëŠ” í•¨ìˆ˜"""
    possible_columns = ['ë§¤ì²´ë§í¬', 'ë§¤ì²´ ë§í¬', 'ë§¤ì²´URL', 'ë§¤ì²´ URL', 'media_link', 'media_url']
    
    for col in possible_columns:
        if col in df.columns:
            return col
    
    # ìœ ì‚¬í•œ ì»¬ëŸ¼ëª… ì°¾ê¸°
    for col in df.columns:
        if 'ë§¤ì²´' in str(col) and ('ë§í¬' in str(col) or 'URL' in str(col)):
            return col
    
    return None

def create_naver_hyperlink(title, naver_url=None, other_url=None, media_url=None):
    """ë„¤ì´ë²„ ë§í¬ë¥¼ í•˜ì´í¼ë§í¬ë¡œ ìƒì„±í•˜ëŠ” í•¨ìˆ˜ (ë§¤ì²´ë§í¬ ì§€ì› ì¶”ê°€)"""
    if pd.isna(title):
        title = "ì œëª© ì—†ìŒ"
    else:
        title = str(title).strip()
    
    # 1ìˆœìœ„: ë„¤ì´ë²„ ë§í¬ê°€ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©
    if pd.notna(naver_url) and str(naver_url).strip() and str(naver_url).strip().lower() != 'nan':
        naver_url = str(naver_url).strip()
        # URLì´ httpë¡œ ì‹œì‘í•˜ì§€ ì•Šìœ¼ë©´ ì¶”ê°€
        if not naver_url.startswith(('http://', 'https://')):
            naver_url = 'https://' + naver_url
        return f'ğŸ“° [{title}]({naver_url})'
    
    # 2ìˆœìœ„: ë§¤ì²´ë§í¬ê°€ ìˆìœ¼ë©´ ì‚¬ìš©
    if pd.notna(media_url) and str(media_url).strip() and str(media_url).strip().lower() != 'nan':
        media_url = str(media_url).strip()
        # URLì´ httpë¡œ ì‹œì‘í•˜ì§€ ì•Šìœ¼ë©´ ì¶”ê°€
        if not media_url.startswith(('http://', 'https://')):
            media_url = 'https://' + media_url
        return f'ğŸ“° [{title}]({media_url})'
    
    # 3ìˆœìœ„: ë„¤ì´ë²„ ë§í¬ê°€ ì—†ìœ¼ë©´ ë‹¤ë¥¸ URL ì‚¬ìš©
    if pd.notna(other_url) and str(other_url).strip() and str(other_url).strip().lower() != 'nan':
        other_url = str(other_url).strip()
        # URLì´ httpë¡œ ì‹œì‘í•˜ì§€ ì•Šìœ¼ë©´ ì¶”ê°€
        if not other_url.startswith(('http://', 'https://')):
            other_url = 'https://' + other_url
        return f'ğŸ”— [{title}]({other_url})'
    
    return f"ğŸ“„ {title}"

def validate_url(url):
    """URL ìœ íš¨ì„± ê²€ì‚¬"""
    if pd.isna(url):
        return False
    
    url = str(url).strip()
    
    # ë¹ˆ ë¬¸ìì—´ì´ë‚˜ 'nan' ì²´í¬
    if not url or url.lower() == 'nan':
        return False
    
    # ê¸°ë³¸ URL íŒ¨í„´ ì²´í¬
    url_pattern = r'https?://[^\s<>"\']+|www\.[^\s<>"\']+|[^\s<>"\']*\.com[^\s<>"\']*'
    return bool(re.match(url_pattern, url, re.IGNORECASE))

def format_article_link(article_row):
    """ê¸°ì‚¬ í–‰ì—ì„œ ë§í¬ ì •ë³´ë¥¼ í¬ë§·íŒ… (ë§¤ì²´ë§í¬ ì§€ì› ì¶”ê°€)"""
    title = article_row.get('ì œëª©', 'ì œëª© ì—†ìŒ')
    naver_url = article_row.get('ë„¤ì´ë²„ URL') or article_row.get('ë„¤ì´ë²„ë§í¬')
    media_url = article_row.get('ë§¤ì²´ URL') or article_row.get('ë§¤ì²´ë§í¬')
    other_url = article_row.get('ê¸°íƒ€ URL')
    
    # 1ìˆœìœ„: ë„¤ì´ë²„ URL í™•ì¸
    if validate_url(naver_url):
        link_url = str(naver_url).strip()
        if not link_url.startswith(('http://', 'https://')):
            link_url = 'https://' + link_url
        return {
            'title': title,
            'url': link_url,
            'source': 'naver',
            'html': f'ğŸ“° [{title}]({link_url})'
        }
    
    # 2ìˆœìœ„: ë§¤ì²´ URL í™•ì¸
    if validate_url(media_url):
        link_url = str(media_url).strip()
        if not link_url.startswith(('http://', 'https://')):
            link_url = 'https://' + link_url
        return {
            'title': title,
            'url': link_url,
            'source': 'media',
            'html': f'ğŸ“° [{title}]({link_url})'
        }
    
    # 3ìˆœìœ„: ê¸°íƒ€ URL í™•ì¸
    if validate_url(other_url):
        link_url = str(other_url).strip()
        if not link_url.startswith(('http://', 'https://')):
            link_url = 'https://' + link_url
        return {
            'title': title,
            'url': link_url,
            'source': 'other',
            'html': f'ğŸ”— [{title}]({link_url})'
        }
    
    # ë§í¬ê°€ ì—†ëŠ” ê²½ìš°
    return {
        'title': title,
        'url': None,
        'source': 'none',
        'html': f"ğŸ“„ {title}"
    }

# ================================
# ìƒˆë¡œìš´ AI ê¸°ëŠ¥ í•¨ìˆ˜ë“¤
# ================================

def generate_today_planning_items(df, industry="ì „ì²´"):
    """ìƒìœ„ ê°€ì¤‘ì¹˜ ê¸°ì‚¬ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ Perplexity APIë¥¼ í˜¸ì¶œí•˜ì—¬ 'ì˜¤ëŠ˜ì˜ ê¸°íš ì•„ì´í…œ' 3ê°œë¥¼ ì œì•ˆ"""
    if df.empty:
        return "ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
    
    # ì—…ê³„ í•„í„°ë§
    if industry != "ì „ì²´":
        df = df[df["ì—…ê³„"] == industry]
    
    # ê°€ì¤‘ì¹˜ ìƒìœ„ ê¸°ì‚¬ 20ê°œê¹Œì§€ ëŒ€ìƒ
    top_articles = (
        df.nlargest(20, "ì „ì²´ê°€ì¤‘ì¹˜") if "ì „ì²´ê°€ì¤‘ì¹˜" in df.columns else df.head(20)
    )
    
    # ê¸°ì‚¬ í…ìŠ¤íŠ¸ ì¶”ì¶œ
    texts = []
    for _, article in top_articles.iterrows():
        title = article.get("ì œëª©", "")
        body = article.get("ì£¼ìš”ë‚´ìš©", "")
        if pd.notna(title) and pd.notna(body):
            texts.append(f"ì œëª©: {title}\në‚´ìš©: {body}")
        elif pd.notna(title):
            texts.append(f"ì œëª©: {title}")
    
    if not texts:
        return "ë¶„ì„í•  ê¸°ì‚¬ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤."
    
    # Perplexity API í˜¸ì¶œìš© í”„ë¡¬í”„íŠ¸ ìƒì„±
    prompt = f"""
    ë‹¤ìŒì€ ì˜¤ëŠ˜ì˜ ì£¼ìš” ë‰´ìŠ¤ ê¸°ì‚¬ë“¤ì…ë‹ˆë‹¤. ì´ ê¸°ì‚¬ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ê¸°ìë“¤ì´ ì¶”ê°€ ì·¨ì¬í•˜ê¸° ì¢‹ì€ 'ê¸°íš ì•„ì´í…œ' 3ê±´ì„ ì œì•ˆí•´ì£¼ì„¸ìš”.
    
    ê¸°ì‚¬ ëª©ë¡:
    {chr(10).join(texts[:15])}
    
    [ë‹µë³€ í˜•ì‹]
    1. ê¸°íš ì•„ì´í…œ ì œëª©:
    - ì·¨ì¬ í¬ì¸íŠ¸:
    - ì˜ˆìƒ ì†ŒìŠ¤:
    
    2. ê¸°íš ì•„ì´í…œ ì œëª©:
    - ì·¨ì¬ í¬ì¸íŠ¸:
    - ì˜ˆìƒ ì†ŒìŠ¤:
    
    3. ê¸°íš ì•„ì´í…œ ì œëª©:
    - ì·¨ì¬ í¬ì¸íŠ¸:
    - ì˜ˆìƒ ì†ŒìŠ¤:
    """
    
    # API í˜¸ì¶œ (ìºì‹œ í™œìš©)
    try:
        from app import call_perplexity_api_cached
        result = call_perplexity_api_cached(prompt, max_age_hours=6)
        return result or "ê¸°íš ì•„ì´í…œ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
    except ImportError:
        return "AI ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. app.py ëª¨ë“ˆì„ í™•ì¸í•´ì£¼ì„¸ìš”."

def generate_monthly_weekly_insights(df, target_month=None):
    """ì›”ë³„Â·ì£¼ì°¨ë³„ í•µì‹¬ ë¬¸ì¥ ìƒì„± (Perplexity API í™œìš©)"""
    if df.empty:
        return {}
    
    # ì›”ë³„ í•„í„°ë§
    if target_month and "ì›”" in df.columns:
        df = df[df["ì›”"] == target_month]
    
    if df.empty:
        return {}
    
    # ì›” ì „ì²´ í•µì‹¬ ë¬¸ì¥ ìƒì„±
    month_texts = []
    for _, row in df.head(50).iterrows():
        title = row.get("ì œëª©", "")
        content = row.get("ì£¼ìš”ë‚´ìš©", "")
        if pd.notna(title):
            month_texts.append(f"{title} {content if pd.notna(content) else ''}")
    
    monthly_prompt = f"""
    ë‹¤ìŒì€ {target_month or 'ì´ë²ˆ ë‹¬'} ì£¼ìš” ë‰´ìŠ¤ë“¤ì…ë‹ˆë‹¤.
    ì´ ë‰´ìŠ¤ë“¤ì˜ í•µì‹¬ ì´ìŠˆë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ ì£¼ì„¸ìš”.
    
    ë‰´ìŠ¤ ë‚´ìš©:
    {' '.join(month_texts)}
    """
    
    # ì£¼ì°¨ë³„ í•µì‹¬ ë¬¸ì¥ ìƒì„±
    weekly_insights = {}
    if "ì£¼ì°¨" in df.columns:
        for week in sorted(df["ì£¼ì°¨"].dropna().unique()):
            week_df = df[df["ì£¼ì°¨"] == week]
            week_texts = []
            for _, row in week_df.head(20).iterrows():
                title = row.get("ì œëª©", "")
                content = row.get("ì£¼ìš”ë‚´ìš©", "")
                if pd.notna(title):
                    week_texts.append(f"{title} {content if pd.notna(content) else ''}")
            
            weekly_prompt = f"""
            ë‹¤ìŒì€ {week} ì£¼ìš” ë‰´ìŠ¤ë“¤ì…ë‹ˆë‹¤.
            ì´ ë‰´ìŠ¤ë“¤ì˜ í•µì‹¬ ì´ìŠˆë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ ì£¼ì„¸ìš”.
            
            ë‰´ìŠ¤ ë‚´ìš©:
            {' '.join(week_texts)}
            """
            
            # API í˜¸ì¶œ
            try:
                from app import call_perplexity_api_cached
                weekly_insights[week] = call_perplexity_api_cached(
                    weekly_prompt, max_age_hours=24
                )
            except ImportError:
                weekly_insights[week] = "AI ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    # ì›”ë³„ í•µì‹¬ ë¬¸ì¥ API í˜¸ì¶œ
    try:
        from app import call_perplexity_api_cached
        monthly_insight = call_perplexity_api_cached(monthly_prompt, max_age_hours=12)
    except ImportError:
        monthly_insight = "AI ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    return {
        "monthly_insight": monthly_insight,
        "weekly_insights": weekly_insights
    }

def find_similar_articles_to_insight(df, insight, top_n=5):
    """í•µì‹¬ ë¬¸ì¥ê³¼ ìœ ì‚¬ë„ê°€ ë†’ì€ ê¸°ì‚¬ nê°œ ë°˜í™˜"""
    if df.empty or not insight:
        return []
    
    # ê¸°ì‚¬ í…ìŠ¤íŠ¸ ì¤€ë¹„
    df = df.copy()
    df["combined_text"] = df.apply(
        lambda row: f"{row.get('ì œëª©', '')} {row.get('ì£¼ìš”ë‚´ìš©', '')}", axis=1
    )
    
    df = df[df["combined_text"].str.strip() != ""]
    
    if df.empty:
        return []
    
    # TF-IDF ìœ ì‚¬ë„ ê³„ì‚°
    try:
        from sklearn.feature_extraction.text import TfidfVectorizer
        from sklearn.metrics.pairwise import cosine_similarity
        
        vectorizer = TfidfVectorizer(max_features=1000)
        corpus = df["combined_text"].tolist() + [insight]
        tfidf_matrix = vectorizer.fit_transform(corpus)
        
        # ìœ ì‚¬ë„ ê³„ì‚° (ë§ˆì§€ë§‰ ìš”ì†Œê°€ insight)
        similarities = cosine_similarity(tfidf_matrix[-1:], tfidf_matrix[:-1]).flatten()
        
        # ìƒìœ„ ìœ ì‚¬ë„ ê¸°ì‚¬ ì„ íƒ
        similar_indices = similarities.argsort()[::-1][:top_n]
        
        results = []
        for idx in similar_indices:
            if similarities[idx] > 0.1:  # ìµœì†Œ ìœ ì‚¬ë„ ì„ê³„ê°’
                results.append({
                    "article": df.iloc[idx],
                    "similarity": similarities[idx]
                })
        
        return results
        
    except ImportError:
        return []

# ================================
# ë©”ì¸ ì „ì²˜ë¦¬ í•¨ìˆ˜
# ================================

def preprocess_dataframe(df):
    """DataFrame ì „ì²˜ë¦¬"""
    if df is None or df.empty:
        return df
    
    try:
        print("ğŸš€ ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘...")
        
        # ê¸°ë³¸ ì „ì²˜ë¦¬
        df = df.dropna(how='all')
        df.columns = df.columns.str.strip()
        df = df.replace('', pd.NA)
        
        # ë‚ ì§œ ì²˜ë¦¬
        if 'ë³´ë„ë‚ ì§œ' in df.columns:
            if 'ì›”' not in df.columns:
                df['ì›”'] = df['ë³´ë„ë‚ ì§œ'].apply(extract_month_from_date)
            if 'ì£¼ì°¨' not in df.columns:
                df['ì£¼ì°¨'] = df['ë³´ë„ë‚ ì§œ'].apply(extract_week_from_date)
        
        # ì—…ê³„ ì»¬ëŸ¼ ìë™ ì¶”ê°€
        df = add_industry_column(df)
        
        # ê°€ì¤‘ì¹˜ ê³„ì‚°
        if 'êµ¬ë¶„(ì¼ë°˜/ë‹¨ë…)' in df.columns:
            df['ê°€ì¤‘ì¹˜'] = df.apply(calculate_article_weight, axis=1)
        
        # ì‹œê°„ ê°€ì¤‘ì¹˜ ê³„ì‚°
        if 'ë³´ë„ë‚ ì§œ' in df.columns:
            df['ì‹œê°„ê°€ì¤‘ì¹˜'] = df['ë³´ë„ë‚ ì§œ'].apply(calculate_time_weight)
        
        # ì „ì²´ ê°€ì¤‘ì¹˜ ê³„ì‚° (ê¸°ì¡´ ê°€ì¤‘ì¹˜ + ì‹œê°„ ê°€ì¤‘ì¹˜)
        if 'ê°€ì¤‘ì¹˜' in df.columns:
            df['ì „ì²´ê°€ì¤‘ì¹˜'] = df['ê°€ì¤‘ì¹˜'] * 0.7 + df['ì‹œê°„ê°€ì¤‘ì¹˜'] * 0.3
        else:
            df['ì „ì²´ê°€ì¤‘ì¹˜'] = df['ì‹œê°„ê°€ì¤‘ì¹˜']
        
        # ë„¤ì´ë²„ ë§í¬ ì²˜ë¦¬ ì»¬ëŸ¼ ì¶”ê°€
        if 'ì œëª©' in df.columns:
            df['ë§í¬ì •ë³´'] = df.apply(lambda row: format_article_link(row), axis=1)
        
        print("âœ… ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ!")
        print(f"ğŸ“Š ìµœì¢… ë°ì´í„°: {len(df)}í–‰, {len(df.columns)}ì—´")
        
        return df
        
    except Exception as e:
        print(f"âŒ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        return df

# ================================
# ìµœì¢… ì´ˆê¸°í™”
# ================================

print("âœ… Google Sheets ìœ í‹¸ë¦¬í‹° ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ")
print("ğŸ“Š konlpy ê¸°ë°˜ í•œêµ­ì–´ ì²˜ë¦¬ ì—”ì§„ ì‚¬ìš©")
print("ğŸš€ êµ¬ê¸€ ì‹œíŠ¸ ë°ì´í„° ìºì‹± ì‹œìŠ¤í…œ í™œì„±í™”")

# ë²„ì „ ì •ë³´
__version__ = "3.3.0-konlpy-cached"
__author__ = "ë‰´ìŠ¤íŒ©í† ë¦¬ ê°œë°œíŒ€"
__description__ = "konlpy ê¸°ë°˜ í•œêµ­ì–´ ë‰´ìŠ¤ ë¶„ì„ + êµ¬ê¸€ ì‹œíŠ¸ ìºì‹± ì‹œìŠ¤í…œ + ë§í¬ í•˜ì´í¼ë§í¬"

print(f"ğŸš€ ëª¨ë“ˆ ë²„ì „: {__version__}")
print(f"ğŸ‘¥ ê°œë°œì: {__author__}")
print(f"ğŸ“ ì„¤ëª…: {__description__}")
